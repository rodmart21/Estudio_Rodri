{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["RPr7lOOQjYJe","tG8F3-2Frhzq"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"re9Lt_eGi1R-"},"source":["# Introduction to Natural Language Processing\n","\n","----------------------------------------------------\n","Machine Learning    \n","\n","*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es*, *Emilio Parrado Hernández emipar@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n","\n","----------------------------------------------------\n"]},{"cell_type":"code","metadata":{"id":"UZTTwuuIN0MH"},"source":["%matplotlib inline\n","# Figures plotted inside the notebook\n","%config InlineBackend.figure_format = 'svg'\n","# High quality figures\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWvDrfYgN13e"},"source":["# Introduction\n","\n","In general, much of the way we communicate today is through text, whether in messaging services, social media and/or email. So, for example, on services/applications such as TripAdvisor, Booking, Amazon, etc., users write reviews of restaurants/businesses, hotels, products to share their opinions about their experience. These reviews, all written in plain text format, contain a lot of information that would be useful to answer business-relevant questions using machine learning methods, for example, to predict the best restaurant in a certain area or even which one best suits my needs, to know if TripAdvisor or Booking reviews are positive or negative (*Sentiment Analysis*), or to analyze news (detect *fake news*).\n","\n","In order to work with this information, from these unformatted texts, we have to extract the necessary information (if possible including semantic content) and vectorize it properly to feed it to machine learning based models.\n","\n","This type of task (preprocessing) is called **Natural Language Processing** (NLP).\n","NLP is a subfield of linguistics, computer science and artificial intelligence that deals with the interactions between computers (or processors) and human language; in particular it encompasses a set of techniques to enable computers to process and analyze large amounts of text.\n","\n","In this course we will look at different techniques and learning models to extract as much information as possible from collections of text and add value to it, but in this session we will focus on the preprocessing of this information. How to make this heterogeneous, sometimes noisy and usually unstructured information into a useful format for our learning models.\n"]},{"cell_type":"markdown","source":["# Pipeline for text processing\n","\n","As we know, ML algorithms process numbers, not words, so we need to transform text into meaningful numbers that contain the relevant information of the documents. This process of converting text to numbers is what we will call **vectorization**.\n","\n","However, in order to have a useful representation, some prior **preprocessing** steps are usually required to clean and homogenize the documents: tokenization, *stop-words* removal, lemmatization, etc.\n","The following figure shows the different steps we must follow to process our documents until they can be used by our learning model:\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/NLP/PipelineNLP.png\" width=\"80%\">\n","\n","Throughout this notebook, we will look at some of the tools we have available in Python to carry out all these steps prior to deploying machine or statistical learning models. Specifically, we will focus on the use of these libraries:\n","* [NLTK, Natural Language ToolKit](https://www.nltk.org/). This library is an excellent NLP library written in Python by experts from both academia and industry. NLTK allows to create applications with textual data quickly, as it provides a set of basic classes to work with data corpora, including text collections (corpora), keyword lists, classes to represent and operate with text type data (documents, phrases, words, ...) and functions to perform common NLP tasks (tokenization, word count, ...). NLTK is going to be of great help for document preprocessing.\n","\n","* [SpaCy](https://spacy.io/). A open source library for advanced natural language processing in Python. SpaCy is specifically designed for production use. Unlike NLTK, SpaCy has an object-oriented structure. For example, when tokenizing text, each token is an object with specific attributes and properties. Spacy supports more than 64 languages, including statistical models already trained for [25 of them](https://spacy.io/usage/models) (including word embeddings and models based on [transformers](https://spacy.io/usage/v3), the latest revolution in NLP).\n","\n","* [Gensim](https://pypi.org/project/gensim/) is another Python library for performing topic modeling (*topic modeling*), document indexing and document information retrieval tasks. It is designed to operate with large amounts of information (with efficient and parallelizable/distributed implementations) and will be of great help for the vectorization of our data corpora once preprocessed.\n","\n","* [Sklearn](https://scikit-learn.org/stable/index.html). Sklearn is a library mainly intended for the design of machine learning models for classification and regression, but it also includes some functionalities for text preprocessing that we will review later.\n","\n","There are also other libraries such as [CoreNLP](https://stanfordnlp.github.io/CoreNLP/), [Huggin Face](https://huggingface.co/transformers/) or packages included in Pytorch, Tensorflow that allow us to perform some of these tasks, but we will introduce them later in other sessions of this course."],"metadata":{"id":"fcagI7IuLikk"}},{"cell_type":"markdown","metadata":{"id":"X22h_atA04h5"},"source":["Let's start this notebook by loading the NLTK library and some of its functionalities. Then, we will choose a corpus of data to analyzing the basic functionalities provided by NLTK, SpaCy, Gensim and Sklearn for document preprocessing and we will see, one by one, what the different steps of our pipeline consist of and how we can implement them."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zycI8RTyKLaR","executionInfo":{"status":"ok","timestamp":1702587379706,"user_tz":-60,"elapsed":6615,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"8afb01e8-910d-414f-8b1b-8ccd8b64c5f7"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"C8JGdmgIKLaW"},"source":["# 1. Loading our data corpus\n","\n","NLTK includes different data corpora to test our tools with. We can find information about all of them at [NLTK corpus](https://www.nltk.org/book/ch02.html).\n"]},{"cell_type":"markdown","metadata":{"id":"OFk2G2d36yzm"},"source":["## The CorpusReader object\n","To start working we are going to use the corpus **movie_reviews**, one of the data corpus included in NLTK and consisting of 2000 text documents with reviews of different movies where it is also indicated if these reviews are positive or negative.\n","\n","The following code cell shows how to load the corpus..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cV1rOSJCBstg","executionInfo":{"status":"ok","timestamp":1702587381152,"user_tz":-60,"elapsed":1463,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"292668ac-acb7-43c3-aacb-e111760149ae"},"source":["from nltk.corpus import movie_reviews\n","nltk.download('movie_reviews')\n","movie_reviews"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["<CategorizedPlaintextCorpusReader in '/root/nltk_data/corpora/movie_reviews'>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"R3Ze2_iK4dY7"},"source":["When loading the corpus, an object of type `CorpusReader`, named `movie_reviews`, is generated with the contents of the corpus. Since a corpus is a collection of documents/texts, we can see which documents compose this corpus using the `.fileids()` function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdLaC5Mu-Lcm","executionInfo":{"status":"ok","timestamp":1702587381153,"user_tz":-60,"elapsed":26,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"3a5e7e64-8d94-4c5f-87df-2078a842c7ac"},"source":["len(movie_reviews.fileids())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oF68SvG-Q_K","executionInfo":{"status":"ok","timestamp":1702587383268,"user_tz":-60,"elapsed":7,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"d2d9694b-2181-447e-cd1f-4e5975fe82a2"},"source":["movie_reviews.fileids()[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['neg/cv000_29416.txt',\n"," 'neg/cv001_19502.txt',\n"," 'neg/cv002_17424.txt',\n"," 'neg/cv003_12683.txt',\n"," 'neg/cv004_12641.txt',\n"," 'neg/cv005_29357.txt',\n"," 'neg/cv006_17022.txt',\n"," 'neg/cv007_4992.txt',\n"," 'neg/cv008_29326.txt',\n"," 'neg/cv009_29417.txt']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"WGYOV4NsUdWO"},"source":["We can also see the categories of this problem with the `.categories()` attribute."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7sqiCPK4eFM","executionInfo":{"status":"ok","timestamp":1702587385934,"user_tz":-60,"elapsed":7,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"c78c0a5d-c271-48ed-f235-3e55c99841f6"},"source":["movie_reviews.categories()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['neg', 'pos']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqEAEXL-pWYe","executionInfo":{"status":"ok","timestamp":1702587387975,"user_tz":-60,"elapsed":11,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"79c5c5b4-7d3a-4bd5-afef-b8f936243932"},"source":["movie_reviews.categories('neg/cv000_29416.txt')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['neg']"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"-tEPerLiKLaX"},"source":["If we want we can access a specific **document** in the corpus and extract its raw content with the `.raw()` function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXSjzPneeiML","executionInfo":{"status":"ok","timestamp":1702587391569,"user_tz":-60,"elapsed":491,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"dfc4f87a-2e01-4a7c-d4de-ba67e1f088c8"},"source":["raw_text = movie_reviews.raw('neg/cv007_4992.txt')\n","print(raw_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["that's exactly how long the movie felt to me . \n","there weren't even nine laughs in nine months . \n","it's a terrible mess of a movie starring a terrible mess of a man , mr . hugh grant , a huge dork . \n","it's not the whole oral-sex/prostitution thing ( referring to grant , not me ) that bugs me , it's the fact that grant is annoying . \n","not just adam sandler-annoying , we're talking jim carrey-annoying . \n","since when do eye flutters and nervous smiles pass for acting ? \n","but , on the other hand , since when do really bad slapstick ( a fistfight in the delivery room culminating in grant's head in joan cusack's lap--a scene he paid $60 to have included in the movie ) and obscene double entendres ( robin williams , the obstetrician , tells grant's pregnant girlfriend she has \" a big pussy , \" referring of course to the size of the cat hairs on her coat , but nonetheless , grant paid $60 to have the exchange included in the movie ) pass for comedy ? \n","nine months is a predictable cookie-cutter movie with no originality in humor or plot . \n","hugh grant plays a successful child psychiatrist . \n","why a child psychologist ? \n","so the scriptwriters could inject the following unfunny exchange : \n","kid : my dad's an asshole . \n","grant ( flutters eyelashes , offers a nervous smile , then responds in his annoying english accent and i-think-i-actually-have- talent attitude ) : could you possibly elaborate on that ? \n","kid : my dad's a _huge_ asshole . \n","more like a hugh asshole , but that's beside the point , which is : nine months includes too many needlessly stupid jokes that get laughs from the ten year olds in the audience while everyone else shakes his or her head in disbelief . \n","so , anyway , grant finds out his girlfriend is pregnant and does his usual reaction ( fluttered eyelashes , nervous smiles ) . \n","this paves the way for every possible pregnancy/child birth gag in the book , especially since grant's equally annoying friend's wife is also pregnant . \n","the annoying friend is played by tom arnold , who provides most of the cacophonous slapstick , none of which is funny , such as a scene where arnold beats up a costumed \" arnie the dinosaur \" ( you draw your own parallels on that one ) in a toy store . \n","the only interesting character in the movie is played by jeff goldblum , who should have hid himself away somewhere after the dreadful hideaway , as an artist with a fear of ( and simultaneous longing for ) commitment . \n","not even robin williams , who plays a russian doctor who has recently decided to switch from veterinary medicine to obstetrics , has much humor . \n","his is a one-joke character-- the old foreign-guy-who-mispronounces-english stereotype ( did someone say yakov smirnov ? \n","that's my favorite vodka , by the way ) , hence the line \" now it's time to take a look at your volvo , \" another nasty but unamusing joke , except this one goes right over the ten year olds' heads , while the adults simultaneously groan . \n","nine months is a complete failure , low on laughs and intelligence and high on loud , unfunny slapstick , failed jokes and other uninspired lunacy . \n","hugh grant's sunset boulevard arrest ( please , no caught-with-his-pants-down jokes ) may bring more people into the theaters , but they certainly won't leave with a smile on their faces , not after 90 minutes of grant's nervous smiles . \n","everything in the movie is so forced , so unauthentic that anyone with an i . q . \n","over 80 ( sorry , hugh ) will know they wasted their money on an unfulfilled desire . \n","but at least they didn't spend 60 bucks for it . \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"7xojODsSNb4G"},"source":["# 2. Preprocessing of the corpus\n","\n","Before transforming the text input data into a vector representation, we need to structure and clean the text, and keep all the information that allows us to capture the semantic content of the corpus. Normally, a vector is obtained for each text, but depending on the application it may be of interest to obtain a vector for each sentence, for each paragraph or even for each word.\n","\n","For this purpose, the typical NLP processing applies the following steps:\n","\n","1. Tokenization\n","2. Cleaning\n","3. Homogenization\n","\n","Note that, although we define this sequence of steps here, we will see that depending on how they are applied, their order may vary."]},{"cell_type":"markdown","metadata":{"id":"dBOayTbJNb4G"},"source":["## 2.1. Tokenization\n","\n","Tokenization is the process of breaking the given text into smaller pieces called tokens. Words, numbers, punctuation marks and others can be considered as tokens.\n","\n","NLTK includes generic functions to perform these operations on any text string. Specifically, it has two functions:\n","- `sent_tokenize`: is a phrase tokenizer. This tokenizer splits a text into a list of sentences. To decide where a sentence starts or ends NLTK has a pre-trained model for the specific language we are working on. We have loaded this model at the beginning with `nltk.download('punkt')`.\n","- `word_tokenize`/`wordpunct_tokenize`: Splits a text into words or other individual characters such as punctuation marks."]},{"cell_type":"markdown","metadata":{"id":"xbCGdSySgtXy"},"source":["Let's see examples of the use of these functions with the following text:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W73SiY9Wgwkh","executionInfo":{"status":"ok","timestamp":1702588182737,"user_tz":-60,"elapsed":9,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"4af6a014-6952-41c7-b0dd-c5038a9e82ac"},"source":["# Define a text as a string\n","texto = 'I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!'\n","print(texto)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! \"Hey\", she'll say!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NYRnSP1g4wN","executionInfo":{"status":"ok","timestamp":1702588186996,"user_tz":-60,"elapsed":1131,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"7ae453be-c6a6-4d65-ff2f-0d3fb6e2aac7"},"source":["# Divide the text into sentences\n","sent=nltk.sent_tokenize(texto)\n","print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"I'm a dog and it's great!\", \"You're cool and Sandy's book is big.\", \"Don't tell her, you'll regret it!\", '\"Hey\", she\\'ll say!']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDA-wzX9sGVc","executionInfo":{"status":"ok","timestamp":1702588189191,"user_tz":-60,"elapsed":10,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"a6f24231-c4cc-4da7-a732-38d1af21e0ff"},"source":["# Divide the text into tokens, using as splitter white spaces and punctuation\n","sent_tokens1=nltk.wordpunct_tokenize(texto)\n","print(sent_tokens1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', \"'\", 'm', 'a', 'dog', 'and', 'it', \"'\", 's', 'great', '!', 'You', \"'\", 're', 'cool', 'and', 'Sandy', \"'\", 's', 'book', 'is', 'big', '.', 'Don', \"'\", 't', 'tell', 'her', ',', 'you', \"'\", 'll', 'regret', 'it', '!', '\"', 'Hey', '\",', 'she', \"'\", 'll', 'say', '!']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp6mC8AtsOUP","executionInfo":{"status":"ok","timestamp":1702588193871,"user_tz":-60,"elapsed":473,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"29203553-9d47-453b-e603-4bedef77ba06"},"source":["# Divide the text into tokens, using as splitter white spaces, but punctuation\n","# of contractions are not included\n","sent_tokens2=nltk.word_tokenize(texto)\n","print(sent_tokens2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', \"'m\", 'a', 'dog', 'and', 'it', \"'s\", 'great', '!', 'You', \"'re\", 'cool', 'and', 'Sandy', \"'s\", 'book', 'is', 'big', '.', 'Do', \"n't\", 'tell', 'her', ',', 'you', \"'ll\", 'regret', 'it', '!', '``', 'Hey', \"''\", ',', 'she', \"'ll\", 'say', '!']\n"]}]},{"cell_type":"markdown","metadata":{"id":"eW6OyNgfiwT_"},"source":["Although it may seem that the `wordpunct_tokenize` and `word_tokenize` functions do the same thing, with this example we see that `wordpunct_tokenize` allows you to separate punctuation marks while `word_tokenize` does not.  Note the difference when splitting `I'm` between the two functions."]},{"cell_type":"markdown","metadata":{"id":"8T-OErnWNb4H"},"source":["## 2.2. Cleaning and homogenization\n","\n","Looking at the tokens in the corpus we can see that there are many tokens with some letters in uppercase and others in lowercase, the same token sometimes appearing in singular and sometimes in plural, or the same verb appearing in different verb tenses. In order to semantically analyze the text, we are interested in **homogenizing** words that are formally different but have the same meaning. In this process, logically, we are losing information about style, meaning, the writer's intention or other aspects; however, what matters to us is the content (subject matter) of the text and in this way we are going to reinforce it for the posterior vectorization.\n","\n","For this we can use the NLTK lemmatization tools. The usual homogenization process consists of the following steps:\n","\n","1. Removal of uppercase and non-alphanumeric characters.\n","\n","2. Cleaning: this preprocessing step consists of removing irrelevant words or **stop words** from the documents.\n","\n","\n","3. Stemming/Lemmatization: remove word endings to preserve the root of words and ignore grammatical information (we remove marks of plurals, gender, verb conjugations, ...).\n","\n","In this step, we can detect typos or misspellings that we could correct.\n","\n","Let's see how to apply each of these steps one by one."]},{"cell_type":"markdown","metadata":{"id":"tClHMtQetMDC"},"source":["#### 2.2.1. Capitalization and punctuation removal\n","\n","Usually, uppercase alphabetic characters are transformed into their corresponding lowercase characters and the non-alphanumeric characters, e.g. punctuation marks, are eliminated."]},{"cell_type":"markdown","metadata":{"id":"iCd3LMgCH8Ra"},"source":["#### 2.2.2. Stop-word removal\n","\n","This preprocessing step consists of removing irrelevant words or **stop words** from documents. The **stop words** are the most common words in a language such as \"the\", \"a\", \"about\", \"is\", \"all\". These words do not have an important meaning and are usually removed from texts. To implement this process, specific libraries containing this list of words are loaded for each language.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdCAXCR9ALKJ","executionInfo":{"status":"ok","timestamp":1702588219231,"user_tz":-60,"elapsed":356,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"124e2c63-c5bc-4011-e089-e2633cd45885"},"source":["from nltk.corpus import stopwords\n","stopwords_en = stopwords.words('english')\n","print('The numer of stopwords is %d' %len(stopwords_en))\n","print(stopwords_en)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The numer of stopwords is 179\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"markdown","metadata":{"id":"dUhw-HCw1O00"},"source":["Note that the English stopwords are designed to remove typical language contractions....."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-Hw8Q30r8CE","executionInfo":{"status":"ok","timestamp":1702588226332,"user_tz":-60,"elapsed":386,"user":{"displayName":"MARIA GONZALEZ GARCIA","userId":"12598603596095616384"}},"outputId":"7ffd44d8-afab-46c7-d011-7dee2cf40940"},"source":["from nltk.corpus import stopwords\n","stopwords_es = stopwords.words('spanish')\n","print('The numer of stopwords is %d' %len(stopwords_es))\n","print(stopwords_es)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The numer of stopwords is 313\n","['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"]}]},{"cell_type":"markdown","metadata":{"id":"l4qC3yWvp1mo"},"source":["Let's see how to apply it with an example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_W-y1Usvp2xa","executionInfo":{"status":"ok","timestamp":1700151070046,"user_tz":-60,"elapsed":11,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"d49528df-48c5-4533-eea2-938862d6c8ef"},"source":["# We define a text and tokenize it\n","text = nltk.wordpunct_tokenize('I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!')\n","print(text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', \"'\", 'm', 'a', 'dog', 'and', 'it', \"'\", 's', 'great', '!', 'You', \"'\", 're', 'cool', 'and', 'Sandy', \"'\", 's', 'book', 'is', 'big', '.', 'Don', \"'\", 't', 'tell', 'her', ',', 'you', \"'\", 'll', 'regret', 'it', '!', '\"', 'Hey', '\",', 'she', \"'\", 'll', 'say', '!']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsnBEDfEqeCj","executionInfo":{"status":"ok","timestamp":1700151070046,"user_tz":-60,"elapsed":8,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"b620437a-2547-4f1e-fe3b-264b77977dc2"},"source":["# We remove the stopwords\n","clean_text1 = [token for token in text if (token not in stopwords_en)]\n","print(clean_text1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', \"'\", 'dog', \"'\", 'great', '!', 'You', \"'\", 'cool', 'Sandy', \"'\", 'book', 'big', '.', 'Don', \"'\", 'tell', ',', \"'\", 'regret', '!', '\"', 'Hey', '\",', \"'\", 'say', '!']\n"]}]},{"cell_type":"markdown","metadata":{"id":"A6ozm2RjKLap"},"source":["#### 2.2.3. Stemming and Lemmatization\n","\n","In common language, words can take different forms indicating gender, quantity, tense (in the case of verbs), concrete forms for nouns/adjectives or adverbs, ... For many applications, it is useful to normalize these forms into some canonical word that facilitates their analysis. There are two ways to accomplish this process:\n","\n","1. The **stemming** process reduces words to their base or root.\n","    \n","2. The goal of **lematization**, like stemmer, is to reduce inflectional forms to a common base form. Unlike stemming, lemmatization does not simply cut inflections. Instead, it uses lexical knowledge bases to derive the correct base forms of words.\n","\n","Here, we are going to focus on the Lemmatization. Lemmatization in NLTK is based on the lexicon from [WordNet](https://wordnet.princeton.edu/). WordNet is a semantically oriented English dictionary, it includes WordNet English with 155,287 words and 117,659 synonym sets.\n","\n","Let's see how lemmatization works with an example:   "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTfrNKpWKLaq","executionInfo":{"status":"ok","timestamp":1700151070046,"user_tz":-60,"elapsed":8,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"c0d2b9e0-8b5b-4929-d360-3095bac76ccd"},"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","\n","\n","text = nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\")\n","\n","# Try lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","lemmas = [lemmatizer.lemmatize(t) for t in text]\n","lematizado1 = \" \".join(lemmas)\n","print(lematizado1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The woman running in the fog passed bunny working a computer scientist .\n"]}]},{"cell_type":"markdown","metadata":{"id":"GNQXBpS5nah7"},"source":["Compare how different processes transform words like `women`, `running` or `computer`."]},{"cell_type":"markdown","metadata":{"id":"ZPcY6x5HsjzP"},"source":["One of the advantages of lemmatization is that the result remains a word, which is more desirable for the presentation of word processing results.\n","\n","However, without using contextual information, `lemmatize()` does not eliminate grammatical differences. For this reason, `running` or `passed` are retained and not replaced by the infinitive `run` or `pass`.\n","\n","As an alternative, we can apply `.lemmatize(word, pos)`, where `pos` is a string code that specifies grammatical function of the words in your sentence (PoS stands for *Part of Speech*). For example, you can check the difference between `wnl.lemmatize('running')` and `wnl.lemmatize('running', pos='v')`.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aVD_Ys1s9ru","executionInfo":{"status":"ok","timestamp":1700151070046,"user_tz":-60,"elapsed":6,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"8b91ac6b-a645-48d4-f669-f29c97a5af7f"},"source":["print(lemmatizer.lemmatize('running'))\n","print(lemmatizer.lemmatize('running', pos='v'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["running\n","run\n"]}]},{"cell_type":"markdown","metadata":{"id":"oKx8y63AAis6"},"source":["## 2.3 Text preprocessing or normalization pipeline\n","\n","Complete the following exercises over the following document."]},{"cell_type":"code","metadata":{"id":"YN3ElSgsKLao","colab":{"base_uri":"https://localhost:8080/","height":184},"executionInfo":{"status":"ok","timestamp":1700145335463,"user_tz":-60,"elapsed":29,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"6d981fa8-3825-4e61-da69-f0915417b987"},"source":["# Get a text\n","movie_text = movie_reviews.raw('neg/cv000_29416.txt')\n","movie_text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn\\'t snag this one correctly . \\nthey seem to have taken this pretty neat concept , but executed it terribly . \\nso what are the problems with the movie ? \\nwell , its main problem is that it\\'s simply too jumbled . \\nit starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what\\'s going on . \\nthere are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \\nnow i personally don\\'t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film\\'s biggest problem . \\nit\\'s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \\nand do they make things entertaining , thrilling or even engaging , in the meantime ? \\nnot really . \\nthe sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn\\'t the make the film all that more entertaining . \\ni guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \\ni mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \\nokay , we get it . . . there \\nare people chasing her and we don\\'t know who they are . \\ndo we really need to see it over and over again ? \\nhow about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \\napparently , the studio took this film away from its director and chopped it up themselves , and it shows . \\nthere might\\'ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \\nthe actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \\nbut my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character\\'s unraveling . \\noverall , the film doesn\\'t stick because it doesn\\'t entertain , it\\'s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \\noh , and by the way , this is not a horror or teen slasher flick . . . it\\'s \\njust packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \\nit also wrapped production two years ago and has been sitting on the shelves ever since . \\nwhatever . . . skip \\nit ! \\nwhere\\'s joblo coming from ? \\na nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"RPr7lOOQjYJe"},"source":["#### Exercise 1: Text Tokenization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0IGfAg7pmXgc"},"source":["Complete the following code cells to split the text into tokens (using the `wordpunct_tokenize` function) and print the resulting tokens. Call `movie_tokens` the output variable."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJv-YzJ8pMih","executionInfo":{"status":"ok","timestamp":1700145335463,"user_tz":-60,"elapsed":28,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"1cb21ddb-6fde-4fd9-9dff-e9c43f1496d7"},"source":["#<SOL>\n","# Divide into words\n","movie_tokens=nltk.wordpunct_tokenize(movie_text)\n","print(movie_tokens)\n","#</SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')']\n"]}]},{"cell_type":"markdown","metadata":{"id":"5SuawzwINb4H"},"source":["##### **Exercise 2**: removing uppercase and non-alphanumeric characters\n","\n","Convert all `movie_tokens` tokens to lowercase (using the `.lower()` method) and remove punctuation or non-alphanumeric tokens (which you can detect with the `.isalnum()` method). This processing can be coded in a single line of code using *list comprehension*.\n","Call `movie_tokens_filtered` the output variable."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5d22gbQuK7P","executionInfo":{"status":"ok","timestamp":1700145335463,"user_tz":-60,"elapsed":25,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"d6aa4ad9-f890-4244-8248-594ccac50d19"},"source":["#<SOL>\n","# List of tokens with puntuation\n","print(movie_tokens)\n","t0 = time.time()\n","movie_tokens_filtered = [el.lower() for el in movie_tokens if el.isalnum()]\n","print('The time needed to remove the punctuation is: %2.4f ms'%(1000*(time.time()-t0)))\n","# Print the text without punctuation\n","print(movie_tokens_filtered)\n","#</SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')']\n","The time needed to remove the punctuation is: 0.4325 ms\n","['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dies', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', 'and', 'has', 'nightmares', 'what', 's', 'the', 'deal', 'watch', 'the', 'movie', 'and', 'sorta', 'find', 'out', 'critique', 'a', 'mind', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', 'mess', 'with', 'your', 'head', 'and', 'such', 'lost', 'highway', 'memento', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', 'and', 'these', 'folks', 'just', 'didn', 't', 'snag', 'this', 'one', 'correctly', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', 'but', 'executed', 'it', 'terribly', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', 'well', 'its', 'main', 'problem', 'is', 'that', 'it', 's', 'simply', 'too', 'jumbled', 'it', 'starts', 'off', 'normal', 'but', 'then', 'downshifts', 'into', 'this', 'fantasy', 'world', 'in', 'which', 'you', 'as', 'an', 'audience', 'member', 'have', 'no', 'idea', 'what', 's', 'going', 'on', 'there', 'are', 'dreams', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', 'there', 'are', 'strange', 'apparitions', 'there', 'are', 'disappearances', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', 'now', 'i', 'personally', 'don', 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', 'which', 'is', 'this', 'film', 's', 'biggest', 'problem', 'it', 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', 'and', 'do', 'they', 'make', 'things', 'entertaining', 'thrilling', 'or', 'even', 'engaging', 'in', 'the', 'meantime', 'not', 'really', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', 'way', 'point', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', 'but', 'it', 'still', 'didn', 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', 'into', 'it', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', 'i', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', 'okay', 'we', 'get', 'it', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', 't', 'know', 'who', 'they', 'are', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', 'apparently', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', 'and', 'it', 'shows', 'there', 'might', 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'in', 'here', 'somewhere', 'but', 'i', 'guess', 'the', 'suits', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', 'would', 'make', 'more', 'sense', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', 'only', 'in', 'a', 'new', 'neighborhood', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', 's', 'unraveling', 'overall', 'the', 'film', 'doesn', 't', 'stick', 'because', 'it', 'doesn', 't', 'entertain', 'it', 's', 'confusing', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', 'oh', 'and', 'by', 'the', 'way', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', 'it', 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', 'whatever', 'skip', 'it', 'where', 's', 'joblo', 'coming', 'from', 'a', 'nightmare', 'of', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'the', 'crow', '9', '10', 'the', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'the', 'others', '9', '10', 'stir', 'of', 'echoes', '8', '10']\n"]}]},{"cell_type":"markdown","metadata":{"id":"0jyiGiS7Ajuf"},"source":["##### **Exercise 3**: Stop-word removal\n","\n","Apply the stop-word removal process on the `moview_reviews` text resulting from the previous filtering process (variable `movie_tokens_filtered` from Exercise 2). Call `movies_tokens_clean` the output variable."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_QYLhQ3BYOK","executionInfo":{"status":"ok","timestamp":1700145335463,"user_tz":-60,"elapsed":19,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"9eec4eb3-c4d4-4ec2-eadc-451f7f0acaf1"},"source":["print('Filtered text:')\n","print(movie_tokens_filtered)\n","#<SOL>\n","movies_tokens_clean = [token for token in movie_tokens_filtered if (token not in stopwords_en)]\n","print('Filtered adn clean text:')\n","print(movies_tokens_clean)\n","#</SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered text:\n","['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dies', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', 'and', 'has', 'nightmares', 'what', 's', 'the', 'deal', 'watch', 'the', 'movie', 'and', 'sorta', 'find', 'out', 'critique', 'a', 'mind', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', 'mess', 'with', 'your', 'head', 'and', 'such', 'lost', 'highway', 'memento', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', 'and', 'these', 'folks', 'just', 'didn', 't', 'snag', 'this', 'one', 'correctly', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', 'but', 'executed', 'it', 'terribly', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', 'well', 'its', 'main', 'problem', 'is', 'that', 'it', 's', 'simply', 'too', 'jumbled', 'it', 'starts', 'off', 'normal', 'but', 'then', 'downshifts', 'into', 'this', 'fantasy', 'world', 'in', 'which', 'you', 'as', 'an', 'audience', 'member', 'have', 'no', 'idea', 'what', 's', 'going', 'on', 'there', 'are', 'dreams', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', 'there', 'are', 'strange', 'apparitions', 'there', 'are', 'disappearances', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', 'now', 'i', 'personally', 'don', 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', 'which', 'is', 'this', 'film', 's', 'biggest', 'problem', 'it', 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', 'and', 'do', 'they', 'make', 'things', 'entertaining', 'thrilling', 'or', 'even', 'engaging', 'in', 'the', 'meantime', 'not', 'really', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', 'way', 'point', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', 'but', 'it', 'still', 'didn', 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', 'into', 'it', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', 'i', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', 'okay', 'we', 'get', 'it', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', 't', 'know', 'who', 'they', 'are', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', 'apparently', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', 'and', 'it', 'shows', 'there', 'might', 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'in', 'here', 'somewhere', 'but', 'i', 'guess', 'the', 'suits', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', 'would', 'make', 'more', 'sense', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', 'only', 'in', 'a', 'new', 'neighborhood', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', 's', 'unraveling', 'overall', 'the', 'film', 'doesn', 't', 'stick', 'because', 'it', 'doesn', 't', 'entertain', 'it', 's', 'confusing', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', 'oh', 'and', 'by', 'the', 'way', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', 'it', 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', 'whatever', 'skip', 'it', 'where', 's', 'joblo', 'coming', 'from', 'a', 'nightmare', 'of', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'the', 'crow', '9', '10', 'the', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'the', 'others', '9', '10', 'stir', 'of', 'echoes', '8', '10']\n","Filtered adn clean text:\n","['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mind', 'fuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'film', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'half', 'way', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', '20', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'might', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'character', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'crow', '9', '10', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'others', '9', '10', 'stir', 'echoes', '8', '10']\n"]}]},{"cell_type":"markdown","metadata":{"id":"v90trBdQ9cyY"},"source":["##### **Exercise 4**: Lemmatization\n","\n","Apply the lemmatization process on the `movie_reviews` text resulting from the previous filtering and cleaning process (variable `movies_tokens_clean` from Exercise 3). Call `movie_tokens_lemmas` the output variable."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d-kfksqrRvs","executionInfo":{"status":"ok","timestamp":1700145335464,"user_tz":-60,"elapsed":16,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"7e2032dd-edaa-4c36-ee66-69207cb7af7b"},"source":["#<SOL>\n","movie_tokens_lemmas = [lemmatizer.lemmatize(t) for t in movies_tokens_clean]\n","print(movie_tokens_lemmas)\n","#</SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['plot', 'two', 'teen', 'couple', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guy', 'dy', 'girlfriend', 'continues', 'see', 'life', 'nightmare', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mind', 'fuck', 'movie', 'teen', 'generation', 'touch', 'cool', 'idea', 'present', 'bad', 'package', 'make', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'film', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'way', 'making', 'type', 'film', 'folk', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problem', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'start', 'normal', 'downshift', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dream', 'character', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparition', 'disappearance', 'looooot', 'chase', 'scene', 'ton', 'weird', 'thing', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'film', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minute', 'make', 'thing', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flick', 'like', 'actually', 'figured', 'half', 'way', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movie', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'vision', '20', 'minute', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'u', 'different', 'scene', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'show', 'might', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'somewhere', 'guess', 'suit', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actor', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'hold', 'throughout', 'entire', 'film', 'actually', 'feeling', 'character', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feel', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kid', 'also', 'wrapped', 'production', 'two', 'year', 'ago', 'sitting', 'shelf', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'crow', '9', '10', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'others', '9', '10', 'stir', 'echo', '8', '10']\n"]}]},{"cell_type":"markdown","metadata":{"id":"tG8F3-2Frhzq"},"source":["#### **Exercise 5**: Function for text normalization\n","\n","Complete the code for the following function so that you can do all the above steps in a single function and then try using it on one of the `movie_reviews` texts."]},{"cell_type":"code","metadata":{"id":"Y6BlL_PAKLaq"},"source":["## Load Modules\n","import string\n","\n","lemmatizer  = WordNetLemmatizer()\n","stopwords_en   = set(nltk.corpus.stopwords.words('english'))\n","punctuation = string.punctuation\n","\n","def normalize(text):\n","    #<SOL>\n","    # change to lower case and remove punctuation\n","    text2 = text.lower().translate(str.maketrans(string.punctuation, ' '*(len(string.punctuation))))\n","    # we tokenize\n","    text_tokens = nltk.word_tokenize(text2)\n","    # we lemmatize and remove stop-words\n","    normalized_text  = [lemmatizer.lemmatize(t) for t in text_tokens if (t not in stopwords_en)]\n","\n","    return normalized_text\n","    #</SOL>\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_62OuAwKLaq","executionInfo":{"status":"ok","timestamp":1700145335464,"user_tz":-60,"elapsed":11,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"25b3d07b-1dbb-4c51-824a-199a2f0747d5"},"source":["movie_text = movie_reviews.raw('neg/cv000_29416.txt')\n","print('Original text (first 200 characters...):')\n","print(movie_text[:200])\n","movie_text_preproc = normalize(movie_text)\n","print('*******************')\n","print('Preprocessed text:')\n","print(movie_text_preproc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text (first 200 characters...):\n","plot : two teen couples go to a church party , drink and then drive . \n","they get into an accident . \n","one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n","w\n","*******************\n","Preprocessed text:\n","['plot', 'two', 'teen', 'couple', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guy', 'dy', 'girlfriend', 'continues', 'see', 'life', 'nightmare', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mind', 'fuck', 'movie', 'teen', 'generation', 'touch', 'cool', 'idea', 'present', 'bad', 'package', 'make', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'film', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'way', 'making', 'type', 'film', 'folk', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problem', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'start', 'normal', 'downshift', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dream', 'character', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparition', 'disappearance', 'looooot', 'chase', 'scene', 'ton', 'weird', 'thing', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'film', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minute', 'make', 'thing', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flick', 'like', 'actually', 'figured', 'half', 'way', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movie', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'vision', '20', 'minute', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'u', 'different', 'scene', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'show', 'might', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'somewhere', 'guess', 'suit', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actor', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'hold', 'throughout', 'entire', 'film', 'actually', 'feeling', 'character', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feel', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kid', 'also', 'wrapped', 'production', 'two', 'year', 'ago', 'sitting', 'shelf', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', '3', '7', '10', 'blair', 'witch', '2', '7', '10', 'crow', '9', '10', 'crow', 'salvation', '4', '10', 'lost', 'highway', '10', '10', 'memento', '10', '10', 'others', '9', '10', 'stir', 'echo', '8', '10']\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZU7AG6p-JNh1"},"source":["#### Preprocessing of the complete data corpus\n","\n","In order to be able to work from now on with all the documents of the data corpus, we will apply our preprocessing pipeline to all the documents of the `moview_reviews` data corpus and save the result in a list variable, called `corpus_prec`, where each element of the list will be a preprocessed text.\n"]},{"cell_type":"code","metadata":{"id":"aGzuNLG-MhGC"},"source":["corpus_prec = []\n","for fileid in movie_reviews.fileids():\n","  text = movie_reviews.raw(fileid)\n","  text_preproc = normalize(text)\n","  corpus_prec.append(text_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3N0vvTVNIKf","executionInfo":{"status":"ok","timestamp":1700145344091,"user_tz":-60,"elapsed":10,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"83ac649b-fdf0-43a2-8d2a-fb4d9e7a9ce4"},"source":["print('Number of documents in the preprocessed corpus:')\n","print(len(corpus_prec))\n","print('**********')\n","print('Some of the elements of the first preprocessed document')\n","print(corpus_prec[0][:20])\n","print('**********')\n","print('Some of the elements of the second preprocessed document')\n","print(corpus_prec[1][:20])\n","print('**********')\n","print('Some of the elements of the third preprocessed document')\n","print(corpus_prec[3][:20])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents in the preprocessed corpus:\n","2000\n","**********\n","Some of the elements of the first preprocessed document\n","['plot', 'two', 'teen', 'couple', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guy', 'dy', 'girlfriend', 'continues', 'see', 'life', 'nightmare', 'deal']\n","**********\n","Some of the elements of the second preprocessed document\n","['happy', 'bastard', 'quick', 'movie', 'review', 'damn', 'y2k', 'bug', 'got', 'head', 'start', 'movie', 'starring', 'jamie', 'lee', 'curtis', 'another', 'baldwin', 'brother', 'william']\n","**********\n","Some of the elements of the third preprocessed document\n","['quest', 'camelot', 'warner', 'bros', 'first', 'feature', 'length', 'fully', 'animated', 'attempt', 'steal', 'clout', 'disney', 'cartoon', 'empire', 'mouse', 'reason', 'worried', 'recent', 'challenger']\n"]}]},{"cell_type":"markdown","source":["## 2.4.  Text preprocessing with SpaCy\n","\n","\n","Text processing with SpaCy is straightforward. We load a pre-trained model for a given language, and pass in any text to be processed. SpaCy will run a series of processes (pipeline) on it and return a `doc` type object.\n","<figure>\n","<center>\n","<img src='https://spacy.io/images/pipeline.svg' width=\"800\"></img>\n","<figcaption>From https://spacy.io/images/pipeline.svg</figcaption></center>\n","</figure>\n","\n","The basic ingredients in SpaCy are:\n","\n","   - `Language`: determined by loading the `model` and associated process pipeline. This `model` transforms text into spaCy objects.\n","   - `Doc`: Iterable sequence of `tokens`. Each `token` is an object with many attributes.\n","   - `Vocab`: Dictionary associated to the model.  \n","\n","\n","Since we are only going to cover some of the basics of SpaCy in this session, additional material can be found in the following resources:\n","\n","- [SpaCy 101 course](https://spacy.io/usage/spacy-101)\n","- [Advanced Tutorial](https://course.spacy.io/en/)\n","\n","\n"],"metadata":{"id":"fjJfV5oO0V00"}},{"cell_type":"markdown","metadata":{"id":"p5huLCvtotQo"},"source":["In the following code, we download and import one of the [pre-trained statistical models for the English language](https://spacy.io/models/en)..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dxoMBTLo9em","outputId":"ec9ec068-18a9-4263-d298-1055cf518a49","scrolled":true,"executionInfo":{"status":"ok","timestamp":1700145363030,"user_tz":-60,"elapsed":18946,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-11-16 14:35:48.846728: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-16 14:35:48.846792: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-16 14:35:48.846825: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-16 14:35:48.855951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-11-16 14:35:50.061977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# Download the model\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxVaJMJGo9em"},"outputs":[],"source":["import spacy\n","\n","# We load the model\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","source":["### SpaCy objects"],"metadata":{"id":"dTWXCha-uxB6"}},{"cell_type":"markdown","metadata":{"id":"aIb43IGOotQr"},"source":["Next, we are going to use the pipeline that we have loaded to analyze a text ..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZkk9921otQr","executionInfo":{"status":"ok","timestamp":1700145374826,"user_tz":-60,"elapsed":9,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"9731ee75-0ea2-4d25-b8be-42e74a186970"},"outputs":[{"output_type":"stream","name":"stdout","text":["I am an engineer. Some day I hope to engineer an electric engine\n"]}],"source":["sentence2 = \"I am an engineer. Some day I hope to engineer an electric engine\"\n","\n","print(sentence2)\n","\n","doc = nlp(sentence2)"]},{"cell_type":"markdown","metadata":{"id":"imtM5Ww1o9en"},"source":["`doc` is an iterable object, composed of objects of type [`token`](https://spacy.io/api/token). In the following loop we print some of the properties of those tokens determined by the pipeline we have loaded, including the POS tag:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHIlk92Xo9en","outputId":"ac78bb25-8911-4be0-fbcd-b369fa833271","executionInfo":{"status":"ok","timestamp":1700145375267,"user_tz":-60,"elapsed":447,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["I I PRP True True False\n","*****\n","am be VBP True True False\n","*****\n","an an DT True True False\n","*****\n","engineer engineer NN True False False\n","*****\n",". . . False False True\n","*****\n","Some some DT True True False\n","*****\n","day day NN True False False\n","*****\n","I I PRP True True False\n","*****\n","hope hope VBP True False False\n","*****\n","to to TO True True False\n","*****\n","engineer engineer VB True False False\n","*****\n","an an DT True True False\n","*****\n","electric electric JJ True False False\n","*****\n","engine engine NN True False False\n","*****\n"]}],"source":["for token in doc:\n","    print(token.text, token.lemma_, token.tag_, token.is_alpha, token.is_stop,token.is_punct)\n","    print('*****')"]},{"cell_type":"markdown","metadata":{"id":"ZRMFkSgoo9eo"},"source":["In table form ..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"2aEzVZ1_o9eo","outputId":"41fc4d0a-0f39-47ac-ca54-65614cff3074","executionInfo":{"status":"ok","timestamp":1700145375267,"user_tz":-60,"elapsed":23,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        Word Word_Lemma POS tag  Is alpha  Is stopword  Is punct\n","0          I          I     PRP      True         True     False\n","1         am         be     VBP      True         True     False\n","2         an         an      DT      True         True     False\n","3   engineer   engineer      NN      True        False     False\n","4          .          .       .     False        False      True\n","5       Some       some      DT      True         True     False\n","6        day        day      NN      True        False     False\n","7          I          I     PRP      True         True     False\n","8       hope       hope     VBP      True        False     False\n","9         to         to      TO      True         True     False\n","10  engineer   engineer      VB      True        False     False\n","11        an         an      DT      True         True     False\n","12  electric   electric      JJ      True        False     False\n","13    engine     engine      NN      True        False     False"],"text/html":["\n","  <div id=\"df-003e7398-8231-4026-a529-e0ed18c925b4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Word_Lemma</th>\n","      <th>POS tag</th>\n","      <th>Is alpha</th>\n","      <th>Is stopword</th>\n","      <th>Is punct</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I</td>\n","      <td>I</td>\n","      <td>PRP</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>am</td>\n","      <td>be</td>\n","      <td>VBP</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>an</td>\n","      <td>an</td>\n","      <td>DT</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>engineer</td>\n","      <td>engineer</td>\n","      <td>NN</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Some</td>\n","      <td>some</td>\n","      <td>DT</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>day</td>\n","      <td>day</td>\n","      <td>NN</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I</td>\n","      <td>I</td>\n","      <td>PRP</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>hope</td>\n","      <td>hope</td>\n","      <td>VBP</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>to</td>\n","      <td>to</td>\n","      <td>TO</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>engineer</td>\n","      <td>engineer</td>\n","      <td>VB</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>an</td>\n","      <td>an</td>\n","      <td>DT</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>electric</td>\n","      <td>electric</td>\n","      <td>JJ</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>engine</td>\n","      <td>engine</td>\n","      <td>NN</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-003e7398-8231-4026-a529-e0ed18c925b4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-003e7398-8231-4026-a529-e0ed18c925b4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-003e7398-8231-4026-a529-e0ed18c925b4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3c6f5e92-7737-4f9f-8aee-835446e3c5ce\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c6f5e92-7737-4f9f-8aee-835446e3c5ce')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3c6f5e92-7737-4f9f-8aee-835446e3c5ce button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":33}],"source":["import pandas as pd\n","spacy_pos_tagged = [(token.text, token.lemma_, token.tag_,token.is_alpha, token.is_stop,token.is_punct) for token in doc]\n","\n","pd.DataFrame(spacy_pos_tagged, columns=['Word','Word_Lemma','POS tag','Is alpha','Is stopword','Is punct'])"]},{"cell_type":"markdown","metadata":{"id":"MGdL2Nnso9eo"},"source":["With `spacy.explain()` we can get a description of the different tags ..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"7Pcl3UJUo9eo","outputId":"00de5a44-bc55-40f8-f957-bc23b180f0e4","executionInfo":{"status":"ok","timestamp":1700145375267,"user_tz":-60,"elapsed":21,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'verb, non-3rd person singular present'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}],"source":["spacy.explain(\"VBP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"g1F0FG3fo9eo","outputId":"b7f57ba2-b622-4175-b6ae-7e588d664510","executionInfo":{"status":"ok","timestamp":1700145375267,"user_tz":-60,"elapsed":20,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'adjective (English), other noun-modifier (Chinese)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["spacy.explain(\"JJ\")"]},{"cell_type":"markdown","metadata":{"id":"ha9cII0Do9en"},"source":["The access to the model **vocabulary** is done through the `.vocab.strings` attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpQXMSiEo9en","outputId":"ef107b15-b74f-4131-acde-b09c233a5eaf","executionInfo":{"status":"ok","timestamp":1700145375268,"user_tz":-60,"elapsed":20,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The size of the dictionary is 84780 words\n","['\\t', '\\t\\t', '\\t\\x0b', '\\t\\x0c', '\\t\\r', '\\t\\x1c', '\\t\\x1d', '\\t\\x1e', '\\t\\x1f', '\\t ', '\\t\\x85', '\\t\\x85\\u1680', '\\t\\xa0', '\\t\\u1680', '\\t\\u2001', '\\t\\u2005', '\\t\\u2006', '\\t\\u2007', '\\t\\u2008', '\\t\\u2009', '\\t\\u200a', '\\t\\u2028', '\\t\\u205f', '\\t\\u3000', '\\n', '\\n\\t', '\\n\\x0b', '\\n\\r', '\\n\\r\\t', '\\n\\x1c', '\\n ', '\\n\\u1680', '\\n\\u2001', '\\n\\u2002', '\\n\\u2003', '\\n\\u2004', '\\n\\u2005', '\\n\\u2006', '\\n\\u2007', '\\n\\u2008', '\\n\\u2009', '\\n\\u200a', '\\n\\u2029', '\\n\\u202f', '\\n\\u205f', '\\n\\u3000', '\\x0b', '\\x0b\\n', '\\x0b\\x0b', '\\x0b\\x0c', '\\x0b\\r', '\\x0b ', '\\x0b\\x85', '\\x0b\\xa0', '\\x0b\\u1680', '\\x0b\\u2001', '\\x0b\\u2002', '\\x0b\\u2003', '\\x0b\\u2005', '\\x0b\\u2005\\u2000', '\\x0b\\u2006', '\\x0b\\u2007', '\\x0b\\u2008', '\\x0b\\u200a', '\\x0b\\u2028', '\\x0b\\u2029', '\\x0b\\u202f', '\\x0b\\u205f\\u2007', '\\x0b\\u3000', '\\x0c', '\\x0c\\t', '\\x0c\\n', '\\x0c\\r', '\\x0c\\x1d', '\\x0c\\x1f', '\\x0c ', '\\x0c\\x85', '\\x0c\\xa0', '\\x0c\\u1680', '\\x0c\\u2001', '\\x0c\\u2002 ', '\\x0c\\u2003', '\\x0c\\u2007', '\\x0c\\u2008', '\\x0c\\u200a', '\\x0c\\u200a\\u2004', '\\x0c\\u2028', '\\x0c\\u2028\\u200a\\u2004', '\\x0c\\u2029', '\\x0c\\u202f', '\\x0c\\u205f', '\\x0c\\u3000', '\\r', '\\r\\t', '\\r\\n', '\\r\\x0b', '\\r\\x0c', '\\r\\x1c', '\\r ', '\\r\\x85', '\\r\\xa0', '\\r\\u1680', '\\r\\u2000', '\\r\\u2001', '\\r\\u2002', '\\r\\u2004', '\\r\\u2005', '\\r\\u2007', '\\r\\u2008', '\\r\\u2028', '\\r\\u2029', '\\r\\u205f', '\\r\\u3000', '\\x1c', '\\x1c\\x0c', '\\x1c\\r', '\\x1c\\x1d', '\\x1c\\x1e', '\\x1c\\x1f', '\\x1c ', '\\x1c\\x85', '\\x1c\\xa0', '\\x1c\\u1680', '\\x1c\\u2000', '\\x1c\\u2001', '\\x1c\\u2003', '\\x1c\\u2006', '\\x1c\\u2007', '\\x1c\\u2008', '\\x1c\\u2008\\u2000', '\\x1c\\u2009', '\\x1c\\u2028', '\\x1c\\u202f', '\\x1c\\u205f', '\\x1c\\u3000', '\\x1d', '\\x1d\\n', '\\x1d\\x0b', '\\x1d\\r', '\\x1d\\x1c', '\\x1d\\x1d', '\\x1d\\x1e', '\\x1d\\x1f', '\\x1d ', '\\x1d\\u2000', '\\x1d\\u2002', '\\x1d\\u2003', '\\x1d\\u2005', '\\x1d\\u2006', '\\x1d\\u2007', '\\x1d\\u2009', '\\x1d\\u200a', '\\x1d\\u200a\\u2000', '\\x1d\\u2029', '\\x1d\\u202f', '\\x1d\\u205f', '\\x1d\\u3000', '\\x1e', '\\x1e\\n', '\\x1e\\x0c', '\\x1e\\x1d', '\\x1e\\x1f', '\\x1e ', '\\x1e\\x85', '\\x1e\\xa0', '\\x1e\\u1680', '\\x1e\\u2000', '\\x1e\\u2003', '\\x1e\\u2004', '\\x1e\\u2008', '\\x1e\\u2009', '\\x1e\\u200a', '\\x1e\\u2028', '\\x1e\\u2029', '\\x1e\\u202f', '\\x1e\\u3000', '\\x1f', '\\x1f\\t', '\\x1f\\n', '\\x1f\\r', '\\x1f\\x1c', '\\x1f\\x1d', '\\x1f ', '\\x1f\\xa0', '\\x1f\\u2000', '\\x1f\\u2001', '\\x1f\\u2004', '\\x1f\\u2006', '\\x1f\\u2007', '\\x1f\\u2008', '\\x1f\\u2009', '\\x1f\\u200a', '\\x1f\\u2028', '\\x1f\\u2029\\u2009', '\\x1f\\u202f', '\\x1f\\u205f', '\\x1f\\u3000', ' ', ' \\n', ' \\x0b']\n"]}],"source":["list_vocab = list(nlp.vocab.strings)\n","\n","print(\"The size of the dictionary is {} words\".format(len(list_vocab)))\n","\n","# First 200\n","print(list_vocab[:200])"]},{"cell_type":"markdown","metadata":{"id":"CpMrHFlJo9eo"},"source":["Regarding **stopping words**, SpaCy includes an extensive list (326 items in English language) that we can customize for our own application if necessary.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwIpM8C3o9eo","outputId":"a9b5063c-d828-40b3-9068-ddaa33b041a5","executionInfo":{"status":"ok","timestamp":1700145375268,"user_tz":-60,"elapsed":17,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of stop words: 326\n","First ten stop words: ['hereby', 'if', 'his', 'itself', 'i', 'whose', 'who', 'herein', 'without', 'show', 'either', 'sometimes', 'whither', 'regarding', 'forty', 'when', 'not', 'always', 'becoming', 'ourselves']\n"]}],"source":["spacy_stopwords = nlp.Defaults.stop_words\n","\n","# Printing the total number of stop words:\n","print('Number of stop words: %d' % len(spacy_stopwords))\n","\n","# Printing first ten stop words:\n","print('First ten stop words: %s' % list(spacy_stopwords)[:20])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-F_XIt6o9eo","outputId":"6abfa130-8033-4e80-d421-0988b929836a","executionInfo":{"status":"ok","timestamp":1700145375268,"user_tz":-60,"elapsed":13,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of stop words: 327\n","Number of stop words: 326\n"]}],"source":["# Add a word to the stopping words set\n","\n","nlp.Defaults.stop_words.add(\"my_new_stopword\")\n","\n","print('Number of stop words: %d' % len(nlp.Defaults.stop_words))\n","\n","# Remove a word from the stopping words set\n","\n","nlp.Defaults.stop_words.remove(\"my_new_stopword\")\n","\n","print('Number of stop words: %d' % len(nlp.Defaults.stop_words))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EuNMbbDlo9ep"},"source":["### Using statistical models in SpaCy for the Spanish language\n","\n","As mentioned above, SpaCy provides pre-trained models for working with the Spanish language. All of them have been trained on the annotated database [AnCora](http://clic.ub.edu/corpus/). This corpus contains 500,000 journalistic texts published in Spanish media.\n","\n","It is advisable to look at the following [documentation](https://spacy.io/models/es).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fquX2Tw9o9ep","outputId":"57fd0fb9-3e54-4f89-a2a0-be9bcff8f873","executionInfo":{"status":"ok","timestamp":1700145399397,"user_tz":-60,"elapsed":23912,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-11-16 14:36:18.065874: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-16 14:36:18.065947: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-16 14:36:18.065986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-16 14:36:19.546972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting es-core-news-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}],"source":["!python -m spacy download es_core_news_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Hxs22zCo9ep","outputId":"48dab689-1031-4c69-f8d0-b2659bd2c016","executionInfo":{"status":"ok","timestamp":1700145401250,"user_tz":-60,"elapsed":1862,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The length of the dictionary is 182191 words. The first 100 are \n","\n","['\\t', '\\t\\t', '\\t\\x0b', '\\t ', '\\t\\x85', '\\t\\u2003', '\\t\\u2007', '\\t\\u2009', '\\t\\u2028', '\\t\\u3000', '\\n', '\\n\\n', '\\n\\r', '\\n\\x1c', '\\n ', '\\n\\u2006', '\\x0b', '\\x0b\\x0b', '\\x0b\\x0c', '\\x0b\\x0c\\x1e', '\\x0b\\x1c', '\\x0b\\x1d', '\\x0b ', '\\x0b\\x85', '\\x0b\\u2002', '\\x0b\\u2007', '\\x0b\\u2008', '\\x0b\\u2009', '\\x0b\\u200a', '\\x0b\\u2029', '\\x0c', '\\x0c\\n', '\\x0c ', '\\x0c\\xa0', '\\x0c\\u2029', '\\x0c\\u205f', '\\r', '\\r\\r', '\\r\\x85', '\\r\\xa0', '\\r\\u1680', '\\r\\u2001', '\\r\\u2002', '\\r\\u2003', '\\r\\u2004', '\\r\\u2005', '\\r\\u2007', '\\r\\u2008', '\\r\\u2009', '\\r\\u200a', '\\r\\u200a ', '\\r\\u205f', '\\x1c', '\\x1c\\n', '\\x1c\\x1c', '\\x1c\\x1e', '\\x1c\\x1f', '\\x1c\\u2002', '\\x1c\\u2004', '\\x1c\\u2008', '\\x1c\\u2029', '\\x1c\\u202f', '\\x1d', '\\x1d\\x0c', '\\x1d\\r', '\\x1d\\x1c', '\\x1d\\x1e', '\\x1d\\xa0', '\\x1d\\u2002', '\\x1d\\u2008', '\\x1d\\u200a', '\\x1d\\u2029', '\\x1d\\u202f\\x1c', '\\x1e', '\\x1e\\t', '\\x1e\\x0b', '\\x1e\\r', '\\x1e\\x1d', '\\x1e\\x85', '\\x1e\\u2000', '\\x1e\\u2002', '\\x1e\\u2003', '\\x1e\\u2006', '\\x1e\\u2008', '\\x1e\\u202f', '\\x1e\\u3000', '\\x1f', '\\x1f\\n', '\\x1f\\x1c', '\\x1f\\u2000', '\\x1f\\u2003', '\\x1f\\u2006', '\\x1f\\u2008', '\\x1f\\u3000', ' ', ' \\t', ' \\x0c', ' \\r', ' \\r\\x1f', ' \\x1d']\n","\n","There are a total of 521 stopping words in the model\n"]}],"source":["nlp = spacy.load(\"es_core_news_sm\")\n","\n","lista_vocab = list(nlp.vocab.strings)\n","\n","print(\"The length of the dictionary is {} words. The first 100 are \\n\".format(len(lista_vocab)))\n","\n","print(lista_vocab[:100])\n","\n","#Printing the total number of stop words:\n","print('\\nThere are a total of {0} stopping words in the model'.format(len(spacy.lang.es.stop_words.STOP_WORDS)))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pSLJf_z9o9ep"},"source":["Again, we can access token information..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"mF0cJ9Zao9ep","outputId":"6f520649-3938-4e26-be50-8f4079208940","executionInfo":{"status":"ok","timestamp":1700145401251,"user_tz":-60,"elapsed":11,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["             0        1           2    3        4       5     6          7   \\\n","Word        Los  bosques  tropicales   de  América  Latina   que  conocemos   \n","Word_Lemma   el   bosque    tropical   de  América  Latina   que    conocer   \n","POS tag     DET     NOUN         ADJ  ADP    PROPN   PROPN  PRON       VERB   \n","\n","             8    9   ...        38         39   40      41      42   43   44  \\\n","Word        hoy   en  ...  artículo  publicado   la  semana  pasada   en   la   \n","Word_Lemma  hoy   en  ...  artículo  publicado   el  semana  pasado   en   el   \n","POS tag     ADV  ADP  ...      NOUN        ADJ  DET    NOUN     ADJ  ADP  DET   \n","\n","                 45       46     47  \n","Word        Revista  Science      .  \n","Word_Lemma  Revista  Science      .  \n","POS tag       PROPN    PROPN  PUNCT  \n","\n","[3 rows x 48 columns]"],"text/html":["\n","  <div id=\"df-4855edb6-2df7-49c5-b3ed-84450d3be21b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Word</th>\n","      <td>Los</td>\n","      <td>bosques</td>\n","      <td>tropicales</td>\n","      <td>de</td>\n","      <td>América</td>\n","      <td>Latina</td>\n","      <td>que</td>\n","      <td>conocemos</td>\n","      <td>hoy</td>\n","      <td>en</td>\n","      <td>...</td>\n","      <td>artículo</td>\n","      <td>publicado</td>\n","      <td>la</td>\n","      <td>semana</td>\n","      <td>pasada</td>\n","      <td>en</td>\n","      <td>la</td>\n","      <td>Revista</td>\n","      <td>Science</td>\n","      <td>.</td>\n","    </tr>\n","    <tr>\n","      <th>Word_Lemma</th>\n","      <td>el</td>\n","      <td>bosque</td>\n","      <td>tropical</td>\n","      <td>de</td>\n","      <td>América</td>\n","      <td>Latina</td>\n","      <td>que</td>\n","      <td>conocer</td>\n","      <td>hoy</td>\n","      <td>en</td>\n","      <td>...</td>\n","      <td>artículo</td>\n","      <td>publicado</td>\n","      <td>el</td>\n","      <td>semana</td>\n","      <td>pasado</td>\n","      <td>en</td>\n","      <td>el</td>\n","      <td>Revista</td>\n","      <td>Science</td>\n","      <td>.</td>\n","    </tr>\n","    <tr>\n","      <th>POS tag</th>\n","      <td>DET</td>\n","      <td>NOUN</td>\n","      <td>ADJ</td>\n","      <td>ADP</td>\n","      <td>PROPN</td>\n","      <td>PROPN</td>\n","      <td>PRON</td>\n","      <td>VERB</td>\n","      <td>ADV</td>\n","      <td>ADP</td>\n","      <td>...</td>\n","      <td>NOUN</td>\n","      <td>ADJ</td>\n","      <td>DET</td>\n","      <td>NOUN</td>\n","      <td>ADJ</td>\n","      <td>ADP</td>\n","      <td>DET</td>\n","      <td>PROPN</td>\n","      <td>PROPN</td>\n","      <td>PUNCT</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 48 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4855edb6-2df7-49c5-b3ed-84450d3be21b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4855edb6-2df7-49c5-b3ed-84450d3be21b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4855edb6-2df7-49c5-b3ed-84450d3be21b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-47b10cc5-da3f-46bd-a26e-a7a1709ef822\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47b10cc5-da3f-46bd-a26e-a7a1709ef822')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-47b10cc5-da3f-46bd-a26e-a7a1709ef822 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":41}],"source":["doc = nlp(\"\"\"Los bosques tropicales de América Latina que conocemos hoy en día, incluida la selva amazónica\n","y la mata atlántica, nacieron gracias al impacto del meteorito que extinguió a los dinosaurios,\n","según revela un artículo publicado la semana pasada en la Revista Science.\"\"\")\n","\n","spacy_pos_tagged = [(token.text, token.lemma_, token.tag_) for token in doc]\n","\n","pd.DataFrame(spacy_pos_tagged, columns=['Word','Word_Lemma','POS tag']).T"]},{"cell_type":"markdown","source":["### Using SpaCy for the text preprocessing\n","\n","As we have seen, using SpaCy, we can directly obtain token lemmas along with some attributes to indicate whether the token is a stop word (.is_stop), alphanumeric (.is_alpha), a punctuation mark (.is_punct) or a digit (.is_digit) . We can directly use this information to preprocess our corpus.\n"],"metadata":{"id":"CUcdan_rM_QL"}},{"cell_type":"markdown","source":["### Exercise 6\n","\n","Rewrite the normalization text function (see Exercise 5) to use the SpaCy funcionalities."],"metadata":{"id":"WqjKPDFhN30W"}},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"AE7M2Tbpvn7n"}},{"cell_type":"code","source":["# <SOL>\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def normalize_Spacy(text):\n","    text2 = nlp(text)\n","    normalized_text = [w.lemma_.lower() for w in text2 if not w.is_stop\n","                  and not w.is_punct and (w.is_alpha or w.is_digit)]\n","    return normalized_text\n","\n","# </SOL>\n"],"metadata":{"id":"02s7aybPOFAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_prec = []\n","for fileid in movie_reviews.fileids():\n","  text = movie_reviews.raw(fileid)\n","  text_preproc = normalize_Spacy(text)\n","  corpus_prec.append(text_preproc)\n"],"metadata":{"id":"VlG-Up9cvmiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUmd8ms2Nb4J"},"source":["# 3. Vectorization\n","\n","Up to this point, we have transformed the raw text collection into a text list, where each text is a homogenized collection of the most relevant words for the corpus analysis. Now, we need to convert this data (a list of token lists) into a numerical representation (a list of vectors, or a matrix).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Su_mnHqTegXo"},"source":["## 3.1 Bag of Words (BoW)\n","\n","**Bag-of-words** is a text representation that describes the occurrence of words within a document.  It is called \"bag-of-words\" because any information about the order or structure of words in the document is discarded. The model is only concerned with whether (and how many times) known words appear in the document, not the order in which they appear.\n","\n","To obtain the BoW representation we can use several libraries. In this section we will start by looking at how to use the [`gensim`](https://pypi.org/project/gensim/) library designed specifically for text processing, allowing very efficient processing for large corpora and including additional useful features. Then we will explain how to do the same process with `sklearn`.\n"]},{"cell_type":"markdown","metadata":{"id":"CeqIS57CM1j1"},"source":["The BoW generation process has two steps:\n","1. **Corpus vocabulary generation**. This vocabulary is generated by means of a **dictionary** that stores in an ordered way a vocabulary of known words (at corpus level).\n","2. **Document vectorization** or BoW generation. A measure of the presence of known words (those in the vocabulary) is calculated by counting the number of times that each word in the dictionary appears in each document.\n","\n","In this way, for each document we will obtain a vector (array) of the dictionary size, where each element of the vector (array) will be associated to a token of the dictionary.\n","\n","Let's see how to implement these steps:\n"]},{"cell_type":"markdown","metadata":{"id":"sH6M2ORpPjUf"},"source":["### Dictionary generation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfBctuyENb4J","executionInfo":{"status":"ok","timestamp":1700145670218,"user_tz":-60,"elapsed":865,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"7efe290c-f4c8-436f-c582-3d6189f76e5c"},"source":["import gensim\n","\n","# Create dictionary of tokens: the input is the preprocessed corpus\n","D = gensim.corpora.Dictionary(corpus_prec)\n","n_tokens = len(D)\n","\n","print('The dictionary contains', n_tokens, 'terms')\n","print('First terms in the dictionary:')\n","for n in range(20):\n","    print(str(n), ':', D[n])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The dictionary contains 30854 terms\n","First terms in the dictionary:\n","0 : 2\n","1 : 20\n","2 : 3\n","3 : accident\n","4 : actor\n","5 : actually\n","6 : ago\n","7 : american\n","8 : apparently\n","9 : apparition\n","10 : applaud\n","11 : arrow\n","12 : assume\n","13 : attempt\n","14 : audience\n","15 : away\n","16 : bad\n","17 : beauty\n","18 : bentley\n","19 : big\n"]}]},{"cell_type":"markdown","metadata":{"id":"wUDcbY1LYGVz"},"source":["As we can see, the dictionary is nothing more than a list of words. But the order of this list will be very important for the vectorization of each document, since we will generate tuples of (`id`, `count`) and the `id` will be the positions of these words in the dictionary."]},{"cell_type":"markdown","metadata":{"id":"OdazZ16qNb4K"},"source":["### Vocabulary customization\n","\n","As the size of the vocabulary increases, so does the vector representation of the documents. In the example above, the vector length of the documents is equal to the number of known words.\n","\n","For a very large corpus, such as thousands of documents, the length of the vector that will represent each document may be thousands or hundreds of thousands of positions.\n","\n","In addition, each document may contain very few of the known words in the vocabulary, making processing difficult.\n","\n","To avoid this, it is advisable to analyze the dictionary and eliminate tokens that are not relevant: punctuation marks that still remain or terms that appear in very few cases (and are therefore uninformative terms) or even tokens that appear in all the documents of the corpus (and are not discriminative).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bBWSTmNYUHO3"},"source":["The Gensim dictionary allows us to filter these terms easily:\n","\n","* `.filter_tokens()` allows us to indicate with the parameter `bad_ids` the list of ids of the words to remove or with `good_ids` the list of the ids of the words to leave in the dictionary (removing the rest).\n","\n","* `.filter_extremes()` allows to remove rare or very frequent words/tokens indicating with the parameters:\n","* `no_below`: keep the number of tokens that are contained in at least `no_below` documents.\n","* `no_above`: keep the percentage (fraction of the total corpus size, not an absolute number) of tokens that are contained in no more than `no_above` documents.\n","* `keep_n`: directly keeps the most frequent `keep_n` tokens.\n","* `keep_tokens`: list of tokens that should remain in the dictionary after being filtered.\n","\n","Let's see how to use these two functionalities..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrIZU0YTTvVt","executionInfo":{"status":"ok","timestamp":1700145670218,"user_tz":-60,"elapsed":7,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"39c38780-4c93-4224-f849-b684067eefda"},"source":["D.filter_tokens(bad_ids=range(8))  # we remove the numbers that appear at the beginning\n","n_tokens = len(D)\n","print('The dictionary contains', n_tokens, 'terms')\n","print('First terms in the dictionary:')\n","for n in range(10):\n","    print(str(n), ':', D[n])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The dictionary contains 30846 terms\n","First terms in the dictionary:\n","0 : apparently\n","1 : apparition\n","2 : applaud\n","3 : arrow\n","4 : assume\n","5 : attempt\n","6 : audience\n","7 : away\n","8 : bad\n","9 : beauty\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hpUv3RrNb4L","executionInfo":{"status":"ok","timestamp":1700145670508,"user_tz":-60,"elapsed":293,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"a189cee1-59fb-4c62-c187-f11d9ea21e4f"},"source":["no_below = 5 # Minimum number of documents to keep a term in the dictionary\n","no_above = .75 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n","\n","D.filter_extremes(no_below=no_below,no_above=no_above, keep_n=1500)\n","n_tokens = len(D)\n","\n","print('The dictionary contains', n_tokens, 'terms')\n","\n","print('First terms in the dictionary:')\n","for n in range(10):\n","    print(str(n), ':', D[n])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The dictionary contains 1500 terms\n","First terms in the dictionary:\n","0 : apparently\n","1 : assume\n","2 : attempt\n","3 : audience\n","4 : away\n","5 : bad\n","6 : beauty\n","7 : big\n","8 : bit\n","9 : break\n"]}]},{"cell_type":"markdown","metadata":{"id":"cWpsHENXWufn"},"source":["This removes all tokens in the dictionary that:\n","1. Are in less than `no_below=5` documents.  \n","2. Are in more than `no_above=0.75` ($75%$) of documents.\n","3. After (1) and (2), save only the first `keep_n` most frequent tokens (or save all if `keep_n=None`).\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"KgqkXRpYNb4L"},"source":["### Vectorization of documents\n","\n","Once our dictionary is defined, the last step is to generate the BoW. To do this, for each document, we have to generate a vector of the vocabulary length and in each position a value with the count or number of times that word appears in the document. In this way, each document is transformed into a list of tuples `(id, n)`, where `id` is the id of the word within the dictionary and `n` the count of that word within the document.\n","\n","To generate this list of tuples we will use the `.doc2bow()` method. In general, `D.doc2bow(token_list)` transforms any list of tokens into a list of `(token_id, n)` tuples, one for each token in `token_list`, where `token_id` is the identifier of the token (according to the `D` dictionary) and `n` is the number of occurrences of that token in `token_list`."]},{"cell_type":"markdown","metadata":{"id":"NJ2SBTYH5Yte"},"source":["Let's see how to calculate the BoW for the first document in the corpus and analyze the output"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFAn81L95BEc","executionInfo":{"status":"ok","timestamp":1700145670508,"user_tz":-60,"elapsed":7,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"3cf56b8c-8568-478a-ce67-467b33894cc4"},"source":["corpus_bow = D.doc2bow(corpus_prec[0])\n","\n","print('Original document (after cleaning):')\n","print(corpus_prec[0])\n","print('Sparse vector representation (first 10 components):')\n","print(corpus_bow[:10])\n","print('Word counts for the document (first 10 components):')\n","list_word_counts = [(D[doc_bow[0]], doc_bow[1]) for doc_bow in corpus_bow[:10]]\n","print(list_word_counts)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original document (after cleaning):\n","['plot', 'teen', 'couple', 'church', 'party', 'drink', 'drive', 'accident', 'guy', 'die', 'girlfriend', 'continue', 'life', 'nightmare', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mind', 'fuck', 'movie', 'teen', 'generation', 'touch', 'cool', 'idea', 'present', 'bad', 'package', 'make', 'review', 'hard', 'write', 'generally', 'applaud', 'film', 'attempt', 'break', 'mold', 'mess', 'head', 'lose', 'highway', 'memento', 'good', 'bad', 'way', 'make', 'type', 'film', 'folk', 'snag', 'correctly', 'take', 'pretty', 'neat', 'concept', 'execute', 'terribly', 'problem', 'movie', 'main', 'problem', 'simply', 'jumbled', 'start', 'normal', 'downshift', 'fantasy', 'world', 'audience', 'member', 'idea', 'go', 'dream', 'character', 'come', 'dead', 'look', 'like', 'dead', 'strange', 'apparition', 'disappearance', 'looooot', 'chase', 'scene', 'ton', 'weird', 'thing', 'happen', 'simply', 'explain', 'personally', 'mind', 'try', 'unravel', 'film', 'clue', 'kind', 'feed', 'film', 'big', 'problem', 'obviously', 'get', 'big', 'secret', 'hide', 'want', 'hide', 'completely', 'final', 'minute', 'thing', 'entertaining', 'thrilling', 'engaging', 'meantime', 'sad', 'arrow', 'dig', 'flick', 'like', 'actually', 'figure', 'half', 'way', 'point', 'strangeness', 'start', 'little', 'bit', 'sense', 'film', 'entertaining', 'guess', 'line', 'movie', 'like', 'sure', 'audience', 'give', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'show', 'melissa', 'sagemiller', 'run', 'away', 'vision', '20', 'minute', 'movie', 'plain', 'lazy', 'okay', 'people', 'chase', 'know', 'need', 'give', 'different', 'scene', 'offer', 'insight', 'strangeness', 'go', 'movie', 'apparently', 'studio', 'take', 'film', 'away', 'director', 'chop', 'show', 'pretty', 'decent', 'teen', 'mind', 'fuck', 'movie', 'guess', 'suit', 'decide', 'turn', 'music', 'video', 'little', 'edge', 'sense', 'actor', 'pretty', 'good', 'wes', 'bentley', 'play', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'big', 'kudo', 'sagemiller', 'hold', 'entire', 'film', 'actually', 'feel', 'character', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excite', 'feel', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'come', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'package', 'look', 'way', 'apparently', 'assume', 'genre', 'hot', 'kid', 'wrap', 'production', 'year', 'ago', 'sit', 'shelf', 'skip', 'joblo', 'come', 'nightmare', 'elm', 'street', '3', 'blair', 'witch', '2', 'crow', 'crow', 'salvation', 'lose', 'highway', 'memento', 'stir', 'echo']\n","Sparse vector representation (first 10 components):\n","[(0, 2), (1, 1), (2, 1), (3, 2), (4, 2), (5, 2), (6, 1), (7, 3), (8, 1), (9, 1)]\n","Word counts for the document (first 10 components):\n","[('apparently', 2), ('assume', 1), ('attempt', 1), ('audience', 2), ('away', 2), ('bad', 2), ('beauty', 1), ('big', 3), ('bit', 1), ('break', 1)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wy_eSClz5mMH"},"source":["#### **Exercise 7**: Get the BoW of all documents in the corpus\n","\n","... and save the output in a list called `corpus_bow` where each element of the list is the BoW of a document, so that the output can then be parsed and represented."]},{"cell_type":"code","metadata":{"id":"7gCE6ZcBNb4L","colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"status":"error","timestamp":1700501651979,"user_tz":-60,"elapsed":11,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"e0424015-2b42-48f3-c3ea-b14b020f91c0"},"source":["#<SOL>\n","corpus_bow = [D.doc2bow(doc) for doc in corpus_prec]\n","#</SOL>"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b7e80a21116c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#<SOL>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_prec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#</SOL>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'corpus_prec' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJxAXOGgNb4L","executionInfo":{"status":"ok","timestamp":1700145670849,"user_tz":-60,"elapsed":7,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"8d2f36d1-1df1-4b38-a096-ad345e011db6"},"source":["n_doc=50\n","print('Original document (after cleaning):')\n","print(corpus_prec[n_doc])\n","print('Sparse vector representation (first 10 components):')\n","print(corpus_bow[n_doc][:10])\n","print('Word counts for the document (first 10 components):')\n","list_word_counts = [(D[doc_bow[0]], doc_bow[1]) for doc_bow in corpus_bow[n_doc][:10]]\n","print(list_word_counts)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original document (after cleaning):\n","['giant', 'movie', 'price', 'worth', 'cost', 'admission', 'free', 'lamely', 'direct', 'michael', 'lehmann', 'picture', 'try', 'comedy', 'reuse', 'giant', 'joke', 'book', 'goliath', 'david', 'punk', 'throwing', 'rock', 'languidly', 'pace', 'movie', 'increasingly', 'lugubrious', 'screenplay', 'david', 'seltzer', 'oman', 'meander', 'sad', 'ending', 'rarely', 'funny', 'convincingly', 'dramatic', 'filmmaker', 'think', 'remain', 'mystery', 'nice', 'visual', 'recommend', 'picture', 'great', 'film', 'comedy', 'belt', 'harry', 'meet', 'sally', 'original', 'city', 'slicker', 'billy', 'crystal', 'show', 'world', 'good', 'oscar', 'host', 'propensity', 'choose', 'hopeless', 'material', 'year', 'father', 'day', 'year', 'giant', 'wonder', 'read', 'script', 'agree', 'ahead', 'project', 'surely', 'read', 'giant', 'show', 'think', 'ad', 'lib', 'success', 'maybe', 'want', 'dramatic', 'actor', 'think', 'movie', 'somber', 'tone', 'charitable', 'thing', 'say', 'ineffectual', 'people', 'probably', 'forget', 'see', 'day', 'joke', 'movie', 'single', 'idea', 'place', 'real', 'life', 'basketball', 'player', 'gheorghe', 'muresan', 'stand', 'seven', 'half', 'foot', 'tall', 'visually', 'striking', 'situation', 'possible', 'sammy', 'crystal', 'agent', 'currently', 'client', 'rescue', 'car', 'accident', 'sweet', 'romanian', 'giant', 'name', 'max', 'muresan', 'throw', 'constant', 'liner', 'god', 'salvage', 'business', 'big', 'foot', 'take', 'car', 'sammy', 'decide', 'see', 'max', 'meal', 'ticket', 'like', 'willing', 'sign', 'max', 'disgusting', 'event', 'like', 'wrestling', 'match', 'half', 'dozen', 'dwarf', 'movie', 'know', 'bound', 'contain', 'putrid', 'lengthy', 'vomiting', 'scene', 'series', 'miss', 'opportunity', 'comedic', 'movie', 'turn', 'cheap', 'maudlin', 'disease', 'movie', 'learn', 'max', 'condition', 'terminal', 'soon', 'die', 'fair', 'movie', 'contain', 'good', 'scene', 'trailer', 'let', 'hope', 'billy', 'get', 'pair', 'read', 'glass', 'sign', 'movie', 'giant', 'run', 'long', '1', '37', 'rate', 'pg', 'violence', 'profanity', 'acceptable', 'kid', 'son', 'jeffrey', 'friend', 'matthew', '9', 'give', 'movie', 'single', 'star', 'good', 'matthew', 'point', 'good', 'amen', 'go', 'comment', 'like', 'way', 'movie', 'change', 'completely', 'middle', 'jeffrey', 'complain', 'particularly', 'gross', 'movie']\n","Sparse vector representation (first 10 components):\n","[(7, 1), (14, 1), (24, 1), (26, 1), (32, 1), (50, 1), (52, 1), (53, 1), (54, 4), (57, 2)]\n","Word counts for the document (first 10 components):\n","[('big', 1), ('completely', 1), ('decide', 1), ('die', 1), ('ending', 1), ('get', 1), ('give', 1), ('go', 1), ('good', 4), ('half', 2)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"id":"w8AChxOQPylP","executionInfo":{"status":"ok","timestamp":1700145671219,"user_tz":-60,"elapsed":373,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"7f0fd283-0a14-47e5-bbc0-7dee9049487d"},"source":["(words, counts) = zip(*list_word_counts)\n","plt.figure(figsize=(10,5))\n","plt.stem(words,counts, use_line_collection = True)\n","plt.xlabel('Words')\n","plt.ylabel('Counts')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-50-d7bdb8515caa>:3: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n","  plt.stem(words,counts, use_line_collection = True)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"608.98125pt\" height=\"321.95625pt\" viewBox=\"0 0 608.98125 321.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-11-16T14:41:10.661758</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 321.95625 \nL 608.98125 321.95625 \nL 608.98125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 284.4 \nL 601.78125 284.4 \nL 601.78125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m2e6f7fe7fb\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"69.144886\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- big -->\n      <g transform=\"translate(61.407386 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-62\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"125.508523\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- completely -->\n      <g transform=\"translate(97.804616 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-63\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n       <use xlink:href=\"#DejaVuSans-6d\" x=\"116.162109\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"213.574219\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"277.050781\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"304.833984\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"366.357422\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"405.566406\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"467.089844\"/>\n       <use xlink:href=\"#DejaVuSans-79\" x=\"494.873047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"181.872159\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- decide -->\n      <g transform=\"translate(165.232315 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"125\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"179.980469\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"207.763672\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"271.240234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"238.235795\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- die -->\n      <g transform=\"translate(230.595952 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"294.599432\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- ending -->\n      <g transform=\"translate(277.447869 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-65\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"61.523438\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"124.902344\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"188.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"216.162109\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"279.541016\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"350.963068\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- get -->\n      <g transform=\"translate(342.752131 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"407.326705\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- give -->\n      <g transform=\"translate(396.727486 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-76\" x=\"91.259766\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"150.439453\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"463.690341\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- go -->\n      <g transform=\"translate(457.456747 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"520.053977\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- good -->\n      <g transform=\"translate(507.58679 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"185.839844\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m2e6f7fe7fb\" x=\"576.417614\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- half -->\n      <g transform=\"translate(567.035582 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-68\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.441406\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Words -->\n     <g transform=\"translate(307.325 312.676562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-57\" d=\"M 213 4666 \nL 850 4666 \nL 1831 722 \nL 2809 4666 \nL 3519 4666 \nL 4500 722 \nL 5478 4666 \nL 6119 4666 \nL 4947 0 \nL 4153 0 \nL 3169 4050 \nL 2175 0 \nL 1381 0 \nL 213 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-57\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"93.001953\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"154.183594\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"193.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"257.023438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path id=\"m63ff7b6d81\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"271.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 275.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"240.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 244.099219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"208.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 212.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"177.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 181.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 149.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"114.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 118.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"82.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 3.0 -->\n      <g transform=\"translate(20.878125 86.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"51.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 3.5 -->\n      <g transform=\"translate(20.878125 55.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#m63ff7b6d81\" x=\"43.78125\" y=\"19.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 4.0 -->\n      <g transform=\"translate(20.878125 23.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- Counts -->\n     <g transform=\"translate(14.798438 163.253125) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"131.005859\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"194.384766\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"257.763672\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"296.972656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path d=\"M 69.144886 271.8 \nL 69.144886 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 125.508523 271.8 \nL 125.508523 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 181.872159 271.8 \nL 181.872159 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 238.235795 271.8 \nL 238.235795 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 294.599432 271.8 \nL 294.599432 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 350.963068 271.8 \nL 350.963068 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 407.326705 271.8 \nL 407.326705 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 463.690341 271.8 \nL 463.690341 208.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 520.053977 271.8 \nL 520.053977 19.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 576.417614 271.8 \nL 576.417614 145.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <defs>\n     <path id=\"m9c3c21e3aa\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #1f77b4\"/>\n    </defs>\n    <g clip-path=\"url(#p5aa79f61dd)\">\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"69.144886\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"125.508523\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"181.872159\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"238.235795\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"294.599432\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"350.963068\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"407.326705\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"463.690341\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"520.053977\" y=\"19.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m9c3c21e3aa\" x=\"576.417614\" y=\"145.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 69.144886 271.8 \nL 576.417614 271.8 \n\" clip-path=\"url(#p5aa79f61dd)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 284.4 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 601.78125 284.4 \nL 601.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 284.4 \nL 601.78125 284.4 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 601.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5aa79f61dd\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"558\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ygjdET3RPQ-m"},"source":["## 3.2 Representation TF-IDF\n","\n","One problem with BoW is that the frequency of very frequent words start to dominate in the document with respect to the rest; for example, very common verbs or terms that are common in the context of the corpus but do not have as much \"information content\" for the model as may be domain-specific words. Doing the vocabulary improvement we have seen that we can remove such frequent words, but this process is quite manual and it is better to have a robust vectorization than this.\n","\n","To this end, the TF-IDF (Term Frequency-Inverse Document Frequency) representation proposes to readjust the frequency of words according to the frequency with which they appear in all documents, so that frequent word scores are penalized if they are also frequent in all documents. To do this, the TF-IDF involves the calculation of two values:\n","\n","**Term Frequency (TF)**.\n","By term frequency $\\text{TF}(w)$ we mean the number of times a given word $w$ occurs in a document (directly the value given by the BoW).\n","\n","Sometimes this factor is redefined by dividing by the total number of words in that document or by the maximum frequency of some term in that document (to penalize long documents):\n","\n","$$ \\text{TF}(w,d) =\\frac{\\text{# times $w$ occurs in document $d$}}{\\text{# total number of words in document $d$}}$$\n","\n","**Inverse Document Frequency (IDF)**.\n","\n","It is a measure of how much information the word $w$ provides, i.e., whether it is common or rare in all documents in the $D$ corpus. It is calculated as follows:\n","$$ \\text{IDF}(w,D) =\\log \\frac{\\text{# documents in the corpus}}{1+\\text{# documents where the word $w$ appears}}$$\n","\n","From these values the **TF-IDF** is calculated as follows:\n","\n","$$\\text{TF-IDF}(w,d,D) = \\text{TF}(w,d) * \\text{IDF}(w,D)$$\n","\n","A high TF-IDF weight is achieved when the word has a high frequency in the document and, at the same time, a low frequency in the corpus; therefore, the weights tend to filter out terms that are common to many documents.\n","\n","Note that, unlike BoW, for TF-IDF we have to learn the encoding jointly with the whole corpus. However, once we have computed the BoW for all documents, learning the TF-IDF model is straightforward using Gensim's [TfidfModel](https://radimrehurek.com/gensim/models/tfidfmodel.html) function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KacBPzeawCw","executionInfo":{"status":"ok","timestamp":1700145671219,"user_tz":-60,"elapsed":13,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"771af5c6-b093-4b68-e94a-2573f79c25be"},"source":["from gensim.models import TfidfModel\n","\n","# fit TFIDF model for all the corpus\n","model = TfidfModel(corpus_bow)\n","\n","# apply model to the first corpus document\n","vector = model[corpus_bow[0]]  # apply model to the first corpus document\n","print(vector)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0.14659036149633042), (1, 0.09538870540852201), (2, 0.046060597055210165), (3, 0.06203266433058073), (4, 0.08304701694536977), (5, 0.04556863376007878), (6, 0.09338913058374114), (7, 0.08836431369760318), (8, 0.04517805412992079), (9, 0.055976335017854394), (10, 0.03065329494696355), (11, 0.1390363788387408), (12, 0.10363730439058788), (13, 0.0522519477164516), (14, 0.053072458004447716), (15, 0.08780110669093072), (16, 0.1018854403333182), (17, 0.07722079493973014), (18, 0.15293725539054068), (19, 0.053950720875055695), (20, 0.10646847986444416), (21, 0.11349485336090287), (22, 0.058556761740826406), (23, 0.07959482844521315), (24, 0.052050720826289476), (25, 0.0603695951378268), (26, 0.053950720875055695), (27, 0.054129434366684166), (28, 0.022913502643376847), (29, 0.06893158248012145), (30, 0.06447793322866029), (31, 0.09032164494174676), (32, 0.06537007496225165), (33, 0.08473362104893632), (34, 0.08330781620483073), (35, 0.13614440131637306), (36, 0.055317185377341815), (37, 0.10455227612670648), (38, 0.06682668619851327), (39, 0.1054954052161841), (40, 0.09061494169644572), (41, 0.06795941570303772), (42, 0.06447793322866029), (43, 0.0584533914000891), (44, 0.02278431688003939), (45, 0.13815437003974204), (46, 0.09470758233393958), (47, 0.09243622656929605), (48, 0.09679811588102749), (49, 0.06723625946901714), (50, 0.023369989674912297), (51, 0.07182373289193754), (52, 0.05159175561542943), (53, 0.036091801814038954), (54, 0.019249342632656395), (55, 0.13178355475832038), (56, 0.039847635762535645), (57, 0.050104080862638796), (58, 0.04111051870364003), (59, 0.044385356230287575), (60, 0.04633734184267871), (61, 0.17144478630107204), (62, 0.05993329913022172), (63, 0.06435257813785994), (64, 0.08522401994256842), (65, 0.0997384387068613), (66, 0.055317185377341815), (67, 0.046129549338455596), (68, 0.019142140457712897), (69, 0.024610206166174996), (70, 0.03951205638483128), (71, 0.04753349921997815), (72, 0.040736550077583804), (73, 0.09225909867691119), (74, 0.05513143161497263), (75, 0.04341536398645125), (76, 0.04849663431254889), (77, 0.0657605168917041), (78, 0.07779728736462246), (79, 0.1454899029376467), (80, 0.07431475350578827), (81, 0.05704166199915884), (82, 0.04236656727816872), (83, 0.028624759431554913), (84, 0.21004042621752214), (85, 0.09371346458359943), (86, 0.06937048177456998), (87, 0.0634891611270606), (88, 0.07296203315314176), (89, 0.09003112829290263), (90, 0.08021822506099408), (91, 0.08401242831643636), (92, 0.025938432841340876), (93, 0.10063427571656826), (94, 0.015784408143901352), (95, 0.02390015254954384), (96, 0.03614408522840621), (97, 0.058556761740826406), (98, 0.24818071940292083), (99, 0.1260072395067872), (100, 0.062413637546723695), (101, 0.09338913058374114), (102, 0.056649972617500204), (103, 0.03809889615370904), (104, 0.09151205775074518), (105, 0.03281244752729303), (106, 0.1469262448241277), (107, 0.09351440303531679), (108, 0.08003362686778809), (109, 0.11082097228067482), (110, 0.0633677776932537), (111, 0.07577431183935585), (112, 0.07760391589053182), (113, 0.07858320932281965), (114, 0.07573447817670734), (115, 0.08172410644176889), (116, 0.08889584009764116), (117, 0.04812602185269424), (118, 0.05046439411202968), (119, 0.33796533205072216), (120, 0.04499124907725138), (121, 0.07380179190188209), (122, 0.025867072443619587), (123, 0.0306809058136134), (124, 0.06779101265862328), (125, 0.0633677776932537), (126, 0.09789940598408887), (127, 0.028237279214137982), (128, 0.02858579022481032), (129, 0.07542917530360058), (130, 0.09904174728649819), (131, 0.07258791948418956), (132, 0.10455227612670648), (133, 0.03890621087817818), (134, 0.02112210735629298)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"8qCGLB9r6bKJ"},"source":["#### **Exercise 8**: Get the TF-IDF of all documents in the corpus.\n","\n","Store the output in a list called `corpus_tfifd` where each element of the list is the TF-IDF of a document, so that the output can then be parsed and represented."]},{"cell_type":"code","metadata":{"id":"kKYbo-AU6a_j"},"source":["#<SOL>\n","# apply model to all corpus document\n","corpus_tfidf = model[corpus_bow]\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6i-3FWkbUY4"},"source":["Let's analyze this transformation in comparison with the BoW."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMMsxdtGbJr6","executionInfo":{"status":"ok","timestamp":1700145671219,"user_tz":-60,"elapsed":10,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"a18970ae-bc13-41e4-9877-b760cb64243d"},"source":["n_doc=50\n","print('Original document (after cleaning):')\n","print(corpus_prec[n_doc])\n","print('Sparse TFIDF vector representation (first 10 components):')\n","print(corpus_tfidf[n_doc][:10])\n","print('Word counts for the document (first 10 components):')\n","list_word_counts = [(D[doc_bow[0]], doc_bow[1]) for doc_bow in corpus_bow[n_doc][:10]]\n","print(list_word_counts)\n","print('TF-IDF for the document (first 10 components):')\n","list_tfidf = [(D[doc_tfidf[0]], doc_tfidf[1]) for doc_tfidf in corpus_tfidf[n_doc][:10]]\n","print(list_tfidf)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original document (after cleaning):\n","['giant', 'movie', 'price', 'worth', 'cost', 'admission', 'free', 'lamely', 'direct', 'michael', 'lehmann', 'picture', 'try', 'comedy', 'reuse', 'giant', 'joke', 'book', 'goliath', 'david', 'punk', 'throwing', 'rock', 'languidly', 'pace', 'movie', 'increasingly', 'lugubrious', 'screenplay', 'david', 'seltzer', 'oman', 'meander', 'sad', 'ending', 'rarely', 'funny', 'convincingly', 'dramatic', 'filmmaker', 'think', 'remain', 'mystery', 'nice', 'visual', 'recommend', 'picture', 'great', 'film', 'comedy', 'belt', 'harry', 'meet', 'sally', 'original', 'city', 'slicker', 'billy', 'crystal', 'show', 'world', 'good', 'oscar', 'host', 'propensity', 'choose', 'hopeless', 'material', 'year', 'father', 'day', 'year', 'giant', 'wonder', 'read', 'script', 'agree', 'ahead', 'project', 'surely', 'read', 'giant', 'show', 'think', 'ad', 'lib', 'success', 'maybe', 'want', 'dramatic', 'actor', 'think', 'movie', 'somber', 'tone', 'charitable', 'thing', 'say', 'ineffectual', 'people', 'probably', 'forget', 'see', 'day', 'joke', 'movie', 'single', 'idea', 'place', 'real', 'life', 'basketball', 'player', 'gheorghe', 'muresan', 'stand', 'seven', 'half', 'foot', 'tall', 'visually', 'striking', 'situation', 'possible', 'sammy', 'crystal', 'agent', 'currently', 'client', 'rescue', 'car', 'accident', 'sweet', 'romanian', 'giant', 'name', 'max', 'muresan', 'throw', 'constant', 'liner', 'god', 'salvage', 'business', 'big', 'foot', 'take', 'car', 'sammy', 'decide', 'see', 'max', 'meal', 'ticket', 'like', 'willing', 'sign', 'max', 'disgusting', 'event', 'like', 'wrestling', 'match', 'half', 'dozen', 'dwarf', 'movie', 'know', 'bound', 'contain', 'putrid', 'lengthy', 'vomiting', 'scene', 'series', 'miss', 'opportunity', 'comedic', 'movie', 'turn', 'cheap', 'maudlin', 'disease', 'movie', 'learn', 'max', 'condition', 'terminal', 'soon', 'die', 'fair', 'movie', 'contain', 'good', 'scene', 'trailer', 'let', 'hope', 'billy', 'get', 'pair', 'read', 'glass', 'sign', 'movie', 'giant', 'run', 'long', '1', '37', 'rate', 'pg', 'violence', 'profanity', 'acceptable', 'kid', 'son', 'jeffrey', 'friend', 'matthew', '9', 'give', 'movie', 'single', 'star', 'good', 'matthew', 'point', 'good', 'amen', 'go', 'comment', 'like', 'way', 'movie', 'change', 'completely', 'middle', 'jeffrey', 'complain', 'particularly', 'gross', 'movie']\n","Sparse TFIDF vector representation (first 10 components):\n","[(7, 0.028375729605954258), (14, 0.051128209619029245), (24, 0.050143902605850166), (26, 0.051974298340702774), (32, 0.06297531754043804), (50, 0.02251385701399641), (52, 0.024850875528493008), (53, 0.017384810107363015), (54, 0.037088330262317726), (57, 0.09653715111137694)]\n","Word counts for the document (first 10 components):\n","[('big', 1), ('completely', 1), ('decide', 1), ('die', 1), ('ending', 1), ('get', 1), ('give', 1), ('go', 1), ('good', 4), ('half', 2)]\n","TF-IDF for the document (first 10 components):\n","[('big', 0.028375729605954258), ('completely', 0.051128209619029245), ('decide', 0.050143902605850166), ('die', 0.051974298340702774), ('ending', 0.06297531754043804), ('get', 0.02251385701399641), ('give', 0.024850875528493008), ('go', 0.017384810107363015), ('good', 0.037088330262317726), ('half', 0.09653715111137694)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"id":"S1mcxp-hcy6G","executionInfo":{"status":"ok","timestamp":1700145671605,"user_tz":-60,"elapsed":391,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"05d21717-2d28-440b-f36a-43171c469e56"},"source":["#Plot BoW\n","(words, counts) = zip(*list_word_counts)\n","plt.figure(figsize=(10,5))\n","plt.stem(words,counts, use_line_collection = True)\n","plt.xlabel('Words')\n","plt.ylabel('Counts')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-54-7f1aab5f1db9>:4: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n","  plt.stem(words,counts, use_line_collection = True)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"608.98125pt\" height=\"321.95625pt\" viewBox=\"0 0 608.98125 321.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-11-16T14:41:11.017400</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 321.95625 \nL 608.98125 321.95625 \nL 608.98125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 284.4 \nL 601.78125 284.4 \nL 601.78125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m973638d6ea\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"69.144886\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- big -->\n      <g transform=\"translate(61.407386 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-62\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"125.508523\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- completely -->\n      <g transform=\"translate(97.804616 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-63\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n       <use xlink:href=\"#DejaVuSans-6d\" x=\"116.162109\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"213.574219\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"277.050781\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"304.833984\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"366.357422\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"405.566406\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"467.089844\"/>\n       <use xlink:href=\"#DejaVuSans-79\" x=\"494.873047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"181.872159\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- decide -->\n      <g transform=\"translate(165.232315 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"125\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"179.980469\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"207.763672\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"271.240234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"238.235795\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- die -->\n      <g transform=\"translate(230.595952 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"294.599432\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- ending -->\n      <g transform=\"translate(277.447869 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-65\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"61.523438\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"124.902344\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"188.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"216.162109\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"279.541016\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"350.963068\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- get -->\n      <g transform=\"translate(342.752131 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"407.326705\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- give -->\n      <g transform=\"translate(396.727486 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-76\" x=\"91.259766\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"150.439453\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"463.690341\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- go -->\n      <g transform=\"translate(457.456747 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"520.053977\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- good -->\n      <g transform=\"translate(507.58679 298.998438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"185.839844\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m973638d6ea\" x=\"576.417614\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- half -->\n      <g transform=\"translate(567.035582 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-68\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.441406\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Words -->\n     <g transform=\"translate(307.325 312.676562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-57\" d=\"M 213 4666 \nL 850 4666 \nL 1831 722 \nL 2809 4666 \nL 3519 4666 \nL 4500 722 \nL 5478 4666 \nL 6119 4666 \nL 4947 0 \nL 4153 0 \nL 3169 4050 \nL 2175 0 \nL 1381 0 \nL 213 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-57\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"93.001953\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"154.183594\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"193.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"257.023438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path id=\"mde982d0f92\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"271.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 275.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"240.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 244.099219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"208.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 212.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"177.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 181.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 149.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"114.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 118.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"82.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 3.0 -->\n      <g transform=\"translate(20.878125 86.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"51.3\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 3.5 -->\n      <g transform=\"translate(20.878125 55.099219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#mde982d0f92\" x=\"43.78125\" y=\"19.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 4.0 -->\n      <g transform=\"translate(20.878125 23.599219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- Counts -->\n     <g transform=\"translate(14.798438 163.253125) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"131.005859\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"194.384766\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"257.763672\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"296.972656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path d=\"M 69.144886 271.8 \nL 69.144886 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 125.508523 271.8 \nL 125.508523 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 181.872159 271.8 \nL 181.872159 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 238.235795 271.8 \nL 238.235795 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 294.599432 271.8 \nL 294.599432 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 350.963068 271.8 \nL 350.963068 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 407.326705 271.8 \nL 407.326705 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 463.690341 271.8 \nL 463.690341 208.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 520.053977 271.8 \nL 520.053977 19.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 576.417614 271.8 \nL 576.417614 145.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <defs>\n     <path id=\"m6b623abd21\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #1f77b4\"/>\n    </defs>\n    <g clip-path=\"url(#p3e9db7e051)\">\n     <use xlink:href=\"#m6b623abd21\" x=\"69.144886\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"125.508523\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"181.872159\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"238.235795\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"294.599432\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"350.963068\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"407.326705\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"463.690341\" y=\"208.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"520.053977\" y=\"19.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m6b623abd21\" x=\"576.417614\" y=\"145.8\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 69.144886 271.8 \nL 576.417614 271.8 \n\" clip-path=\"url(#p3e9db7e051)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 284.4 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 601.78125 284.4 \nL 601.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 284.4 \nL 601.78125 284.4 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 601.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3e9db7e051\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"558\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":509},"id":"ESHTVrHxc1yk","executionInfo":{"status":"ok","timestamp":1700145671890,"user_tz":-60,"elapsed":292,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"e8a7460e-e392-419c-9dff-162808bd93b8"},"source":["#Plot TF-IDF\n","(words, counts) = zip(*list_tfidf)\n","plt.figure(figsize=(10,5))\n","plt.stem(words,counts, use_line_collection = True)\n","plt.xlabel('Words')\n","plt.ylabel('Counts')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-55-daa1748c8849>:4: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n","  plt.stem(words,counts, use_line_collection = True)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"615.34375pt\" height=\"322.194869pt\" viewBox=\"0 0 615.34375 322.194869\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-11-16T14:41:11.282654</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 322.194869 \nL 615.34375 322.194869 \nL 615.34375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 284.638619 \nL 608.14375 284.638619 \nL 608.14375 7.438619 \nL 50.14375 7.438619 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m4bf3534908\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"75.507386\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- big -->\n      <g transform=\"translate(67.769886 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-62\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"131.871023\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- completely -->\n      <g transform=\"translate(104.167116 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-63\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n       <use xlink:href=\"#DejaVuSans-6d\" x=\"116.162109\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"213.574219\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"277.050781\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"304.833984\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"366.357422\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"405.566406\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"467.089844\"/>\n       <use xlink:href=\"#DejaVuSans-79\" x=\"494.873047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"188.234659\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- decide -->\n      <g transform=\"translate(171.594815 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"125\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"179.980469\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"207.763672\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"271.240234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"244.598295\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- die -->\n      <g transform=\"translate(236.958452 299.237056) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"91.259766\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"300.961932\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- ending -->\n      <g transform=\"translate(283.810369 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-65\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"61.523438\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"124.902344\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"188.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"216.162109\"/>\n       <use xlink:href=\"#DejaVuSans-67\" x=\"279.541016\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"357.325568\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- get -->\n      <g transform=\"translate(349.114631 299.237056) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"413.689205\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- give -->\n      <g transform=\"translate(403.089986 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-76\" x=\"91.259766\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"150.439453\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"470.052841\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- go -->\n      <g transform=\"translate(463.819247 299.237056) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"526.416477\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- good -->\n      <g transform=\"translate(513.94929 299.237056) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-67\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"185.839844\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m4bf3534908\" x=\"582.780114\" y=\"284.638619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- half -->\n      <g transform=\"translate(573.398082 299.237056) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-68\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"124.658203\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.441406\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Words -->\n     <g transform=\"translate(313.6875 312.915181) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-57\" d=\"M 213 4666 \nL 850 4666 \nL 1831 722 \nL 2809 4666 \nL 3519 4666 \nL 4500 722 \nL 5478 4666 \nL 6119 4666 \nL 4947 0 \nL 4153 0 \nL 3169 4050 \nL 2175 0 \nL 1381 0 \nL 213 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-57\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"93.001953\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"154.183594\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"193.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"257.023438\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path id=\"ma3136d9280\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"272.038619\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.00 -->\n      <g transform=\"translate(20.878125 275.837837) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"219.830739\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.02 -->\n      <g transform=\"translate(20.878125 223.629957) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"167.622859\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.04 -->\n      <g transform=\"translate(20.878125 171.422077) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"115.414979\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.06 -->\n      <g transform=\"translate(20.878125 119.214197) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"63.207099\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.08 -->\n      <g transform=\"translate(20.878125 67.006317) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#ma3136d9280\" x=\"50.14375\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 14.798438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- Counts -->\n     <g transform=\"translate(14.798437 163.491744) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"131.005859\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"194.384766\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"257.763672\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"296.972656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path d=\"M 75.507386 272.038619 \nL 75.507386 197.966784 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 131.871023 272.038619 \nL 131.871023 138.573847 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 188.234659 272.038619 \nL 188.234659 141.143276 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 244.598295 272.038619 \nL 244.598295 136.365222 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 300.961932 272.038619 \nL 300.961932 107.648228 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 357.325568 272.038619 \nL 357.325568 213.268581 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 413.689205 272.038619 \nL 413.689205 207.168042 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 470.052841 272.038619 \nL 470.052841 226.657415 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 526.416477 272.038619 \nL 526.416477 175.223464 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n    <path d=\"M 582.780114 272.038619 \nL 582.780114 20.038619 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <defs>\n     <path id=\"mbe5b737fc1\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #1f77b4\"/>\n    </defs>\n    <g clip-path=\"url(#p1fca73f25e)\">\n     <use xlink:href=\"#mbe5b737fc1\" x=\"75.507386\" y=\"197.966784\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"131.871023\" y=\"138.573847\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"188.234659\" y=\"141.143276\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"244.598295\" y=\"136.365222\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"300.961932\" y=\"107.648228\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"357.325568\" y=\"213.268581\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"413.689205\" y=\"207.168042\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"470.052841\" y=\"226.657415\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"526.416477\" y=\"175.223464\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#mbe5b737fc1\" x=\"582.780114\" y=\"20.038619\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 75.507386 272.038619 \nL 582.780114 272.038619 \n\" clip-path=\"url(#p1fca73f25e)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 284.638619 \nL 50.14375 7.438619 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 608.14375 284.638619 \nL 608.14375 7.438619 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 284.638619 \nL 608.14375 284.638619 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.438619 \nL 608.14375 7.438619 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1fca73f25e\">\n   <rect x=\"50.14375\" y=\"7.438619\" width=\"558\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"zGgPmALDeZ2w"},"source":["Compare both representations... Which words had more/less weight in the BoW representation? And in the TF-IDF representation?"]},{"cell_type":"markdown","metadata":{"id":"YX4JxTJiu4x_"},"source":["## 3.3 BOW and TF-IDF in sklearn\n","\n","Sklearn also includes functions for tokenization and vectorization of documents. Specifically, it has the functions:\n","* [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) which implements both tokenization and word count (BoW) in a single class.\n","* [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) which is responsible for obtaining the TF-IDF representation from a BoW representation.\n","\n","* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) which is equivalent to using `CountVectorizer()` followed by `TfidfTransformer()`.\n","\n","Let's see how these methods work and what they allow us to do."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSd8DrHgxOAe","executionInfo":{"status":"ok","timestamp":1700145671890,"user_tz":-60,"elapsed":25,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"024e94f7-dc60-48fd-87aa-6d0b943c0889"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# We start from a raw corpus\n","corpus_example = [\n","     'This is the first document of my book.',\n","     'This document is a new document.',\n","     'And this is the document 3.',\n","     'Is this the best document??']\n","\n","# Define the CountVectorizer method\n","vectorizer = CountVectorizer()\n","# and fit it!\n","vectorizer.fit(corpus_example)\n","# Now, we can obtain the bow of any text (or the corpus)\n","X = vectorizer.transform(corpus_example)\n","print(X.toarray()) # By default, it is a sparse matrix"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 1 1 1 1 0 1 1 1]\n"," [0 0 0 2 0 1 0 1 0 0 1]\n"," [1 0 0 1 0 1 0 0 0 1 1]\n"," [0 1 0 1 0 1 0 0 0 1 1]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJk3Cgkq04m6","executionInfo":{"status":"ok","timestamp":1700145671891,"user_tz":-60,"elapsed":21,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"c44abbe8-309f-455a-fcdb-a63efe0e5d5a"},"source":["# We can access to the vocabulary\n","vectorizer.vocabulary_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'this': 10,\n"," 'is': 5,\n"," 'the': 9,\n"," 'first': 4,\n"," 'document': 3,\n"," 'of': 8,\n"," 'my': 6,\n"," 'book': 2,\n"," 'new': 7,\n"," 'and': 0,\n"," 'best': 1}"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"X8D9mT4F0uec"},"source":["The default configuration tokenizes every text, converts everything to lowercase and extracts words of at least 2 letters (we see that it has removed `'a'` and `'3'` as they are single character tokens); it also removes punctuation."]},{"cell_type":"markdown","metadata":{"id":"9YFBpqJM1EQw"},"source":["If we want, `CountVectorizer` lets us include some additional functionalities such as:\n","* Include a list of *stop words* with the `stop_words` parameter.\n","* Refine the dictionary in a similar way as Gensim does with the following parameters:\n","  * `min_df`: threshold to ignore terms appearing in less than `min_df` documents.\n","  * `max_df`: threshold to ignore terms appearing in more than `max_df` documents.  \n","  * `max_features`: number of terms to include in the dictionary (keeps the most frequent `max_features` words).\n","\n","\n","In the following example we see how to include a filtering of *stop words*.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-Kzw6oKxUbD","executionInfo":{"status":"ok","timestamp":1700145671891,"user_tz":-60,"elapsed":17,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"4780d008-5383-49dc-8a63-8b45840fe1e9"},"source":["# Adding n-grams of minimum length 1 and maximum length of 2\n","vectorizer2 = CountVectorizer(stop_words='english')\n","X2 = vectorizer2.fit_transform(corpus_example)\n","print(vectorizer2.get_feature_names_out())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['best' 'book' 'document' 'new']\n"]}]},{"cell_type":"markdown","metadata":{"id":"_73jABCtKxlT"},"source":["Despite all these facilities, it may happen that we want to include additional steps in our preprocessing (treatment of accents, contractions, numbers, ...) and for this it is not enough to configure these parameters. In this case we have two options:\n","* Use NLTK + Gensim.\n","* Include our preprocessing with NLTK on the sklearn class.\n","\n","To carry out this second option, the easiest way is to define a class or object in charge of applying the desired preprocessing and pass it to `CountVectorizer` in the `analyzer` parameter. Let's see how to apply it with our example."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ID1wAuodK0lv","executionInfo":{"status":"ok","timestamp":1700145671892,"user_tz":-60,"elapsed":15,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"1d8bb8c7-c489-42ae-d2d8-b8ba91468aa0"},"source":["# create a custom analyzer class\n","class MyTextPreprocessing(object):\n","\n","    def __init__(self):\n","        ## Load Modules\n","        self.lemmatizer  = WordNetLemmatizer()\n","        self.stopwords_en   = set(nltk.corpus.stopwords.words('english'))\n","        self.punctuation = string.punctuation\n","\n","    # allow the class instance to be called just like\n","    # a function that applies the preprocessing\n","    def __call__(self, text):\n","        # change to lower case and remove punctuation\n","        text2 = text.lower().translate(str.maketrans(string.punctuation, ' '*(len(string.punctuation))))\n","        # we tokenize\n","        text_tokens = nltk.word_tokenize(text2)\n","        # we lemmatize and remove stop-words\n","        normalized_text  = [lemmatizer.lemmatize(t) for t in text_tokens if (t not in stopwords_en)]\n","\n","        return normalized_text\n","\n","analyzer = MyTextPreprocessing()\n","custom_vec = CountVectorizer(analyzer=analyzer)\n","X_example = custom_vec.fit_transform(corpus_example)\n","tokens = custom_vec.get_feature_names_out()\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['3' 'best' 'book' 'document' 'first' 'new']\n"]}]},{"cell_type":"markdown","metadata":{"id":"-ZddXLBhEfPl"},"source":["If we apply it to our corpus of documents"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"QNmBM-F-Ee9H","executionInfo":{"status":"error","timestamp":1700503085795,"user_tz":-60,"elapsed":366,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"f1054035-2329-42c6-9932-8d1186364b24"},"source":["# We read the entire corpus\n","corpus = []\n","for fileid in movie_reviews.fileids():\n","  text = movie_reviews.raw(fileid)\n","  corpus.append(text)\n","\n","# We include the cleaning of the dictionary\n","no_below = 10 # Minimum number of documents to keep a term in the dictionary\n","no_above = .8 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n","keep_n = 2000\n","\n","analyzer = MyTextPreprocessing()\n","custom_vec = CountVectorizer(analyzer=analyzer, min_df= no_below, max_df =no_above, max_features= keep_n)\n","X_bow = custom_vec.fit_transform(corpus)\n","tokens = custom_vec.get_feature_names_out()\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1a96f5245998>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We read the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"]}]},{"cell_type":"code","metadata":{"id":"luteiPpdGeuZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700145681874,"user_tz":-60,"elapsed":25,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"ed5987ab-2325-4b95-d5d7-f42da3db94ad"},"source":["print('Original document')\n","print(corpus[0][:100])\n","print('Sparse vector representation (first 10 components):')\n","print(X_bow[0,:].data[:10])\n","print('Pair of words a count values (first 10 components):')\n","list_word_counts = [(tokens[index], X_bow[0,index]) for index in X_bow[0,:].indices[:10]]\n","print(list_word_counts)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original document\n","plot : two teen couples go to a church party , drink and then drive . \n","they get into an accident . \n","\n","Sparse vector representation (first 10 components):\n","[1 2 4 1 2 1 1 1 3 1]\n","Pair of words a count values (first 10 components):\n","[('plot', 1), ('two', 2), ('teen', 4), ('couple', 1), ('go', 2), ('church', 1), ('party', 1), ('drive', 1), ('get', 3), ('accident', 1)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"NSOwyME5K99i"},"source":["If we want to obtain the TF-IDF from the BoW, we can use the function `TfidfTransformer`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzhlBtONRFY1","executionInfo":{"status":"ok","timestamp":1700145681874,"user_tz":-60,"elapsed":21,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"030d3fd8-2fed-4f51-9f8a-1e0079a91879"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","vectorizerTFIDF = TfidfTransformer()\n","vectorizerTFIDF.fit(X_example)\n","X_tfidf_example =  vectorizerTFIDF.transform(X_example)\n","print(X_tfidf_example.toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.66338461 0.34618161 0.66338461 0.        ]\n"," [0.         0.         0.         0.722056   0.         0.69183461]\n"," [0.88654763 0.         0.         0.46263733 0.         0.        ]\n"," [0.         0.88654763 0.         0.46263733 0.         0.        ]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"0cOzwVeDSRAP"},"source":["We can include everything in a pipeline\n","\n","*Note: A **pipeline** is a sklearn method that allows us to define a sequence of functions and work with them as a single one, facilitating the training, validation and testing of models composed by sequences of methods.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahiwSA1SQzUA","executionInfo":{"status":"ok","timestamp":1700145681875,"user_tz":-60,"elapsed":18,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"a33282cb-cc72-4299-9fdd-fae61240a326"},"source":["from sklearn.pipeline import Pipeline\n","# Define the steps of the pipeline\n","pipe = Pipeline([('count', CountVectorizer()),\n","                  ('tfid', TfidfTransformer())])\n","# Train all the steps of the pipeline\n","pipe.fit(corpus_example)\n","# Get new outputs of the overall pipeline\n","pipe.transform(corpus_example).toarray()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.43750519, 0.22830836, 0.43750519,\n","        0.22830836, 0.43750519, 0.        , 0.43750519, 0.27925389,\n","        0.22830836],\n","       [0.        , 0.        , 0.        , 0.64308448, 0.        ,\n","        0.32154224, 0.        , 0.61616842, 0.        , 0.        ,\n","        0.32154224],\n","       [0.67049706, 0.        , 0.        , 0.34989318, 0.        ,\n","        0.34989318, 0.        , 0.        , 0.        , 0.42796959,\n","        0.34989318],\n","       [0.        , 0.67049706, 0.        , 0.34989318, 0.        ,\n","        0.34989318, 0.        , 0.        , 0.        , 0.42796959,\n","        0.34989318]])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"2FcWKBeDK96h"},"source":["Or directly use the `TfidfVectorizer` class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0-okUg_SrEz","executionInfo":{"status":"ok","timestamp":1700145681875,"user_tz":-60,"elapsed":15,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"3a959daf-312c-474b-dc01-953ad048d4a8"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit_transform(corpus_example).toarray()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.43750519, 0.22830836, 0.43750519,\n","        0.22830836, 0.43750519, 0.        , 0.43750519, 0.27925389,\n","        0.22830836],\n","       [0.        , 0.        , 0.        , 0.64308448, 0.        ,\n","        0.32154224, 0.        , 0.61616842, 0.        , 0.        ,\n","        0.32154224],\n","       [0.67049706, 0.        , 0.        , 0.34989318, 0.        ,\n","        0.34989318, 0.        , 0.        , 0.        , 0.42796959,\n","        0.34989318],\n","       [0.        , 0.67049706, 0.        , 0.34989318, 0.        ,\n","        0.34989318, 0.        , 0.        , 0.        , 0.42796959,\n","        0.34989318]])"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"SMfLXuVOu7M2"},"source":["# 4. Text classification with *Naive Bayes*\n","\n","Once we have the vector representation of our documents, we can use this representation to train a learning model.  There are many models that we can use in this problem to classify our reviews into positive and negative, but we will now look at the Naive Bayes model, since it is a fairly common and efficient strategy for document classification despite being called \"naive\".\n","\n","If our movie reviews contain the terms *enchanting*, *great*, *awesome*, *surprising*, what is the probability of the review being positive, is it greater than the probability of being negative?\n","\n","Basically a Bayesian classifier will learn the probabilities that the review is bad/good in light  of the words that make it up (the a posteriori probabilities of each class), i.e., it will learn:\n","* Probability that the $i$-th document is positive given the content of the document (the words that make it up, its BoW, TF-IDF, ...): $ P(y_i=1|{\\bf x}_i)$\n","* Probability that the document $i$ is negative given its content: $ P(y_i=-1|{\\bf x}_i)$\n","\n","And, then, the classifier will decide that the $i$-th document belongs to the positive ($m=1$) or negative ($m=-1$) class (or in a general multi-class case to $m=1, \\ldots, M$) by applying a MAP criterion:\n","\n","$$\\hat{y}_i = argmax_m P(y_i=m|{\\bf x}_i)$$\n","\n","To calculate these a posteriori probabilities, Bayes' Theorem is applied, so that:\n","$$ P(y=m|{\\bf x}) = \\frac{P(y=m) p({\\bf x}|y=m)}{p({\\bf x})}$$\n","\n","The Navie Bayes model simplifies the calculation of these probabilities by considering the class likelihood independent over all document words, that is,\n","\n","$$p({\\bf x}|y=m) = \\prod_{d=1}^{D}{p(x_d|y=m) } $$\n","\n","Which leads to the MAP classifier can be obtained as:\n","\n","$$\\hat{y} = argmax_m P(y=m|{\\bf x}) = argmax_m  P(y=m) \\prod_{d=1}^{D}{p(x_d|y=m)}$$\n","\n","\n","This assumption of independence is almost never true in the documents we want to classify, since the language, grammatical rules and syntax usually generate correlations between words.\n","However, in spite of that *Naive Bayes* classifiers work quite well for document classification. This is mainly because by decoupling the distributions of each word we only have to estimate parameters from one-dimensional distributions, which allows us to estimate the necessary parameters with little training data. In addition, this makes considerably faster than other more sophisticated methods.\n"]},{"cell_type":"markdown","metadata":{"id":"RIv6JOrVbQu4"},"source":["Sklearn includes several implementations of Naive Bayes classifiers considering different types of distributions, but for working with vector representations of texts, the most common implementations are the following:\n","* Multinomial Naive Bayes\n","* Complement Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"RGYNtlVDgNXy"},"source":["### 4.1 *Multinomial Naive Bayes*\n","\n","The classifier [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) directly estimates the probabilities of the word $d$ appearing in a document of the class $y$, i.e,\n","\n","$$ \\hat{p}(x_d|y=m) = \\frac{N_{d,m} + \\alpha}{N_m + \\alpha D} $$\n","\n","where $N_{d,m}$ is the number of times the word $d$ appears in all documents of class $m$, $N_m = \\sum_{d=1}^D N_{d,m}$ is the total number of words in all documents of class $m$ and $\\alpha$ is a smoothing parameter to avoid divisions by zero. The values of $N_{d,m}$ can be obtained directly from the BoW representation of each document, but using TF-IDF representations also gives good results."]},{"cell_type":"markdown","metadata":{"id":"6eG92uo4_5RZ"},"source":["### 4.2 *Complement Naive Bayes*\n","\n","[`ComplementNB`](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes) is an adaptation of the previous algorithm to work on the complementary probabilities of each class; thus, it tends to provide more stable estimates of the probabilities of each word, mainly in unbalanced problems, tending to improve considerably the previous version.\n","\n","In this case the model decides according to the rule:\n","\n","$$\\hat{y} = argmin_m P(y \\neq m|{\\bf x}) = argmax_m  P(y \\neq m) \\prod_{d=1}^{D}{p_c(x_d|m)}$$\n","\n","where $p_c(x_d|m)$ is the complementary probability of class $m$ and is approximated by:\n","\n","$$ p_c(x_d|m) = \\frac{w_{d,m}}{\\sum_{m' = 1}^M w_{d,m'}}$$\n","\n","being $w_{d,m}$ the complementary weight of the word $d$ in the class $m$ given by:\n","\n","$$w_{d,m} = \\log{ \\frac{\\sum_{m' \\neq m} N_{d,m'} + \\alpha_d}{\\sum_{d'} \\sum_{m' \\neq m} N_{d',m'} + \\sum_{d'} \\alpha_{d'}}} $$\n","\n","where as before the values of $N_{d,m}$ can be obtained directly from the BoW or TFIDF representation of each document."]},{"cell_type":"markdown","metadata":{"id":"lof131am4AnM"},"source":["Let's see how to use these classifiers with our data corpus. To do this, let's start by loading the problem labels."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvGNLhHzyCry","executionInfo":{"status":"ok","timestamp":1700145681875,"user_tz":-60,"elapsed":12,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"e134cc33-c422-4c48-8fd8-cd49e028d471"},"source":["Y = []\n","for fileid in movie_reviews.fileids():\n","  if movie_reviews.categories(fileid)[0]=='neg':\n","    Y.append(-1)\n","  else:\n","    Y.append(1)\n","Y = np.array(Y)\n","print(Y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1 -1 -1 ...  1  1  1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"pm_Llf0bjAxq"},"source":["Secondly, if we use the vectorization we have done with Gensim, in order to use the sklearn libraries, we have to convert our vector representation into numpy arrays. To do this, gensim includes two functions: [corpus2dense](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2dense.html), [corpus2csc](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2csc.html).\n"]},{"cell_type":"code","metadata":{"id":"YngbiGdNg8sw"},"source":["from gensim.matutils import corpus2dense, corpus2csc\n","\n","n_tokens = len(D)\n","num_docs = len(corpus_bow)\n","# Convert BoW representacion\n","corpus_bow_dense = corpus2dense(corpus_bow, num_terms=n_tokens, num_docs=num_docs).T\n","corpus_bow_sparse = corpus2csc(corpus_bow, num_terms=n_tokens, num_docs=num_docs).T\n","# Convert TFIDF representacion\n","corpus_tfidf_dense = corpus2dense(corpus_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n","corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms=n_tokens, num_docs=num_docs).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPlWj4QRlUwS"},"source":["Let's see what these transformations have done, for example, for the BoW representation:"]},{"cell_type":"code","metadata":{"id":"pDvsDI0hiTEx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700145682926,"user_tz":-60,"elapsed":19,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"f7b16419-5afc-492c-9d88-bee9fc73367c"},"source":["print(corpus_bow[50])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(7, 1), (14, 1), (24, 1), (26, 1), (32, 1), (50, 1), (52, 1), (53, 1), (54, 4), (57, 2), (65, 1), (66, 1), (68, 1), (69, 1), (92, 1), (96, 1), (101, 1), (103, 1), (104, 1), (105, 2), (108, 2), (118, 1), (120, 1), (122, 1), (123, 1), (127, 1), (129, 1), (131, 1), (134, 2), (154, 1), (156, 1), (162, 1), (166, 1), (174, 1), (182, 2), (223, 1), (227, 1), (228, 1), (242, 2), (243, 2), (260, 1), (280, 2), (294, 1), (299, 1), (308, 1), (317, 1), (320, 1), (334, 2), (354, 1), (360, 1), (367, 1), (373, 1), (409, 2), (410, 1), (415, 1), (456, 1), (473, 1), (500, 1), (513, 3), (514, 1), (554, 1), (565, 1), (583, 1), (591, 1), (605, 1), (606, 1), (612, 2), (620, 1), (652, 2), (663, 2), (671, 1), (689, 1), (702, 1), (703, 1), (704, 1), (720, 1), (723, 1), (729, 1), (734, 3), (736, 1), (739, 1), (746, 1), (754, 2), (771, 2), (774, 1), (783, 1), (784, 1), (786, 1), (788, 1), (793, 1), (805, 1), (806, 2), (811, 1), (812, 1), (821, 1), (824, 1), (828, 1), (856, 1), (877, 1), (903, 1), (920, 1), (932, 1), (949, 1), (988, 1), (1015, 6), (1021, 1), (1025, 1), (1027, 1), (1032, 1), (1050, 1), (1051, 1), (1078, 1), (1091, 1), (1110, 1), (1115, 1), (1121, 1), (1128, 1), (1130, 1), (1180, 1), (1200, 1), (1201, 2), (1222, 2), (1253, 1), (1331, 1), (1346, 1), (1358, 1), (1359, 1), (1360, 1)]\n"]}]},{"cell_type":"code","metadata":{"id":"uXQzeE7IhEqt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700145682926,"user_tz":-60,"elapsed":17,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"7e4baac4-9074-43d5-b68b-7ffba7cffa68"},"source":["print(corpus_bow_dense[50,:20])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","metadata":{"id":"0JTqEb-Gi2Rf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700145682927,"user_tz":-60,"elapsed":14,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"27608749-03f0-4af7-e8f4-95301ef8acbe"},"source":["print(corpus_bow_sparse[50,:20])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 7)\t1.0\n","  (0, 14)\t1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"U1NscM9Mk_cY"},"source":["In general, when we have a large corpus of data, we will be interested in handling the sparse representation of the data to save on computational cost. If in our ML processing we use sklearn we will be able to work with this sparse format since most classifiers/regressors can work with both data (dense and sparse).\n"]},{"cell_type":"markdown","metadata":{"id":"A_08m6UK5kVp"},"source":["#### **Exercise 9**: Training a *Navie Bayes* and a *Complement Navie Bayes* classifier\n","\n","Complete the following code to train and evaluate the *Navie Bayes* classifiers seen earlier. The code begins by splitting the data sets into training and test so that you can evaluate the classifier's performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vwa8apVBXsX8","executionInfo":{"status":"ok","timestamp":1700145682927,"user_tz":-60,"elapsed":12,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"310c3b18-1ec1-4093-b461-d3c07140450e"},"source":["from sklearn.naive_bayes import MultinomialNB, ComplementNB\n","from sklearn.model_selection import train_test_split\n","# Code for BoW representation\n","\n","# Divide the data into train test partitions\n","id_tot = np.arange(corpus_bow_sparse.shape[0])\n","id_train, id_test = train_test_split(id_tot, test_size=0.4, random_state=42)\n","\n","X_train = corpus_bow_sparse[id_train,:]\n","X_test = corpus_bow_sparse[id_test,:]\n","y_train = Y[id_train]\n","y_test = Y[id_test]\n","\n","# <SOL>\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","print('Accuracy Naive Bayes')\n","print(clf.score(X_test, y_test))\n","\n","clf = ComplementNB()\n","clf.fit(X_train, y_train)\n","print('Accuracy Complement Naive Bayes')\n","print(clf.score(X_test, y_test))\n","\n","# </SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Naive Bayes\n","0.81125\n","Accuracy Complement Naive Bayes\n","0.81125\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWIO18eR6sRG","executionInfo":{"status":"ok","timestamp":1700145682927,"user_tz":-60,"elapsed":9,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"2b75bd7a-7902-4aef-98f7-886e0f1df8f8"},"source":["# Code for TF-IDF representation\n","\n","# Divide the data into train test partitions\n","X_train = corpus_tfidf_sparse[id_train,:]\n","X_test = corpus_tfidf_sparse[id_test,:]\n","\n","# <SOL>\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","print('Accuracy Naive Bayes')\n","print(clf.score(X_test, y_test))\n","\n","clf = ComplementNB()\n","clf.fit(X_train, y_train)\n","print('Accuracy Complement Naive Bayes')\n","print(clf.score(X_test, y_test))\n","# </SOL>"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Naive Bayes\n","0.81875\n","Accuracy Complement Naive Bayes\n","0.81625\n"]}]},{"cell_type":"markdown","metadata":{"id":"0z26HQyk7Pbe"},"source":["Being a binary problem, there is no difference between the Naive Bayes implementation and its complementary one. Moreover, in this case, we also see no difference between using the BoW representation and the TF-IDF, but as we will see throughout the course the TF-IDF representation usually provides better results."]},{"cell_type":"markdown","metadata":{"id":"64pjt0btUgh4"},"source":["## 4.3 Data normalization?\n","\n","So far we have always seen in our ML pipeline that we should normalize the data by columns (by features) before passing it to the learning model.\n","\n","But working with text vector representations like BoW or TF-IDF, do we need to normalize? Although there is no closed answer, the most accepted idea is that BoW or TF-IDF representations or other NLP transformations that we will see later should be left as they are for best results.\n","\n","This is because the normalization of BoW variables is not considered natural, we lose the importance of each word within the document. While the TF-IDF encoding is already considered to be normalized; note that:\n","* The TF calculation normalizes each document to length 1 (row normalization), eliminating dependency on longer or shorter documents.\n","* Second, the IDF is an inter-document normalization that gives less weight to common terms and more to rare ones (column normalization), normalizing (weighting) each word with the inverse frequency of the corpus.\n","\n","So the TF-IDF is intended to be used in its direct form in an algorithm.\n","\n","However, as we will see, algorithms such as K-means which are usually very sensitive to feature scaling, IDF weighting helps to improve the clustering result and therefore better results are usually obtained with TF-IDF representations than with BOW."]},{"cell_type":"markdown","metadata":{"id":"WRqFJx2jHQs0"},"source":["## 4.4 Calculating distances between BoW and TF-IDF representations\n","\n","Once we have the vector representation of our documents (each of the rows of the BoW or TFIDF matrix), many of our ML models will need to compute similarities between them, such as a K-NN algorithm for classification/regression or a K-means for clustering. For this, when working with BoW or TFIDF features, where the magnitude of the vectors does not matter, **cosine similarity** is often used as a metric.\n","\n","We might assume that when a word (e.g., *science*) appears more frequently in document 1 than in document 2, that document 1 is more related to the *science* topic. However, it could also be the case that we are working with documents of unequal lengths (Wikipedia articles, for example). In this case, *science* is likely to appear more in document 1 just because it is much longer than document 2. The cosine similarity corrects for this.\n","\n","For this reason, when working with BoW or TF-IDF encoded documents, cosine similarity tends to be used. If $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are two TF-IDF vectors, the cosine similarity is calculated as follows:\n","\n","\n","$$\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2) = \\frac{\\mathbf{v}_1^T~\\mathbf{v}_2}{||\\mathbf{v}_1||_2 ||\\mathbf{v}_2||}$$\n","\n","Or, rather the cosine distance defined as $1-\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2)$ is used, i.e.,\n","\n","$$d_\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2) = 1- \\frac{\\mathbf{v}_1^T~\\mathbf{v}_2}{||\\mathbf{v}_1||_2 ||\\mathbf{v}_2||}$$\n","\n","\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/NLP/NLP1.png\" width=\"100%\">\n","\n","And as we can see in the following figure, their results have nothing to do with the Euclidean distance (they don't depend on the length of the vectors)\n","\n","<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/NLP/NLP2.png\" width=\"60%\">\n","\n","\n","Another aspect we have to take into account is that the cosine distance is not a distance as such (it does not satisfy the triangular inequality) and default implementations of our learning models such as sklearn's K-NN or K-means use the default Euclidean distance and do not allow to include the cosine distance.\n","\n","To solve this problem we have two options:\n","* Use other implementations such as NLTK's K-means which allows us to use the cosine distance. Although the problem with this implementation is that it does not allow us to work with sparse matrices.\n","* Rescale our data to be able to use implementations based on the Euclidean distance...\n","\n","Let's see how to do the latter...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NYQwYUbjh37h"},"source":["## 4.5 K-NN with *cosine* distance\n","\n","As we have just mentioned, we cannot use the K-means or K-NN libraries of sklearn with BOW or TF-IDF representations because they only allow us to use the Euclidean distance.\n","\n","However, when our vectors $x$ and $y$ are normalized ($||x||^2 = x^Tx = 1$), their Euclidean distances $||x-y||^2$ and cosine $d(x,y)$ are related by the following equality:\n","\n","$$||x-y||^2 = x^Tx + y^Ty - 2 x^Ty = 2 (1 - x^Ty) = 2 d(x,y)$$\n","\n","In this case, using the Euclidean distance will give us the same results as the cosine distance.\n","\n","Note that this normalization is similar to the TF calculation that compensates in the BoW representation for the length of the documents.\n","\n","So let's start by normalizing our data so that each vector has unit norm (we are going to do it on the sparse representations)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sATYDcPiLPj"},"outputs":[],"source":["from scipy import sparse\n","\n","def normalize_sparse_vector(s):\n","  norm_s = np.array(np.sqrt(s.multiply(s).sum(1)))\n","  pos_zero = np.where(np.sqrt(s.multiply(s).sum(1))==0)[0]\n","  norm_s[pos_zero] = 1\n","  return s.multiply(sparse.csr_matrix(1/norm_s))\n","\n","corpus_bow_sparse_norm = normalize_sparse_vector(corpus_bow_sparse)\n","corpus_tfidf_sparse_norm = normalize_sparse_vector(corpus_tfidf_sparse)"]},{"cell_type":"markdown","metadata":{"id":"PFELzI9Hz2zs"},"source":["Now let's classify our documents with sklearn's K-NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuJZ4ZjpPQ-r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700145718714,"user_tz":-60,"elapsed":35792,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"73b8ff63-db88-4540-a676-4bd749e8676e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy K-NN without normalization\n","0.58875\n","Accuracy K-NN with normalization (cosine distance)\n","0.705\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# Code for BoW representation\n","\n","# Divide the data into train test partitions\n","X_train = corpus_bow_sparse[id_train,:]\n","X_test = corpus_bow_sparse[id_test,:]\n","\n","X_train_norm = corpus_bow_sparse_norm[id_train,:]\n","X_test_norm = corpus_bow_sparse_norm[id_test,:]\n","\n","# <SOL>\n","clf = KNeighborsClassifier()\n","clf.fit(X_train, y_train)\n","print('Accuracy K-NN without normalization')\n","print(clf.score(X_test, y_test))\n","\n","clf = KNeighborsClassifier()\n","clf.fit(X_train_norm, y_train)\n","print('Accuracy K-NN with normalization (cosine distance)')\n","print(clf.score(X_test_norm, y_test))\n","# </SOL>"]},{"cell_type":"code","source":["# Code for TFIDF representation\n","\n","# Divide the data into train test partitions\n","X_train = corpus_tfidf_sparse[id_train,:]\n","X_test = corpus_tfidf_sparse[id_test,:]\n","\n","X_train_norm = corpus_tfidf_sparse_norm[id_train,:]\n","X_test_norm = corpus_tfidf_sparse_norm[id_test,:]\n","\n","# <SOL>\n","clf = KNeighborsClassifier()\n","clf.fit(X_train, y_train)\n","print('Accuracy K-NN without normalization')\n","print(clf.score(X_test, y_test))\n","\n","clf = KNeighborsClassifier()\n","clf.fit(X_train_norm, y_train)\n","print('Accuracy K-NN with normalization (cosine distance)')\n","print(clf.score(X_test_norm, y_test))\n","# </SOL>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qk_CT3-KStoy","executionInfo":{"status":"ok","timestamp":1700145756330,"user_tz":-60,"elapsed":37627,"user":{"displayName":"VANESSA GOMEZ VERDEJO","userId":"10847227554119461925"}},"outputId":"fe1dd406-6904-4bcf-9f02-ffd6036289e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy K-NN without normalization\n","0.6575\n","Accuracy K-NN with normalization (cosine distance)\n","0.6575\n"]}]}]}